{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf4c9f8f",
   "metadata": {},
   "source": [
    "## TODO \n",
    "- maybe integrate the arxiv api and the similarity computation together (you can use the user query to use in the arxiv api)\n",
    "- somehow combine a chatbot with the retrieved papers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa94b86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marko\\miniconda3\\envs\\nlp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcbae39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar papers to your query:\n",
      "ID: 324\n",
      "Title: Personalized Emphasis Framing for Persuasive Message Generation\n",
      "Similarity: 0.6874\n",
      "Summary: In this paper, we present a study on personalized emphasis framing which can\n",
      "be used to tailor the content of a message to enhance its appeal to different\n",
      "individuals. With this framework, we directly model content selection decisions\n",
      "based on a set of psychologically-motivated domain-independent personal traits\n",
      "including personality (e.g., extraversion and conscientiousness) and basic\n",
      "human values (e.g., self-transcendence and hedonism). We also demonstrate how\n",
      "the analysis results can be used in automated personalized content selection\n",
      "for persuasive message generation.\n",
      "--------------------------------------------------------------------------------\n",
      "ID: 490\n",
      "Title: A Survey of Personalized Large Language Models: Progress and Future Directions\n",
      "Similarity: 0.4751\n",
      "Summary: Large Language Models (LLMs) excel in handling general knowledge tasks, yet\n",
      "they struggle with user-specific personalization, such as understanding\n",
      "individual emotions, writing styles, and preferences. Personalized Large\n",
      "Language Models (PLLMs) tackle these challenges by leveraging individual user\n",
      "data, such as user profiles, historical dialogues, content, and interactions,\n",
      "to deliver responses that are contextually relevant and tailored to each user's\n",
      "specific needs. This is a highly valuable research topic, as PLLMs can\n",
      "significantly enhance user satisfaction and have broad applications in\n",
      "conversational agents, recommendation systems, emotion recognition, medical\n",
      "assistants, and more. This survey reviews recent advancements in PLLMs from\n",
      "three technical perspectives: prompting for personalized context (input level),\n",
      "finetuning for personalized adapters (model level), and alignment for\n",
      "personalized preferences (objective level). To provide deeper insights, we also\n",
      "discuss current limitations and outline several promising directions for future\n",
      "research. Updated information about this survey can be found at the\n",
      "https://github.com/JiahongLiu21/Awesome-Personalized-Large-Language-Models.\n",
      "--------------------------------------------------------------------------------\n",
      "ID: 305\n",
      "Title: The Dark Patterns of Personalized Persuasion in Large Language Models: Exposing Persuasive Linguistic Features for Big Five Personality Traits in LLMs Responses\n",
      "Similarity: 0.4482\n",
      "Summary: This study explores how the Large Language Models (LLMs) adjust linguistic\n",
      "features to create personalized persuasive outputs. While research showed that\n",
      "LLMs personalize outputs, a gap remains in understanding the linguistic\n",
      "features of their persuasive capabilities. We identified 13 linguistic features\n",
      "crucial for influencing personalities across different levels of the Big Five\n",
      "model of personality. We analyzed how prompts with personality trait\n",
      "information influenced the output of 19 LLMs across five model families. The\n",
      "findings show that models use more anxiety-related words for neuroticism,\n",
      "increase achievement-related words for conscientiousness, and employ fewer\n",
      "cognitive processes words for openness to experience. Some model families excel\n",
      "at adapting language for openness to experience, others for conscientiousness,\n",
      "while only one model adapts language for neuroticism. Our findings show how\n",
      "LLMs tailor responses based on personality cues in prompts, indicating their\n",
      "potential to create persuasive content affecting the mind and well-being of the\n",
      "recipients.\n",
      "--------------------------------------------------------------------------------\n",
      "ID: 314\n",
      "Title: Personalized Causal Graph Reasoning for LLMs: A Case Study on Dietary Recommendations\n",
      "Similarity: 0.4280\n",
      "Summary: Large Language Models (LLMs) effectively leverage common-sense knowledge for\n",
      "general reasoning, yet they struggle with personalized reasoning when tasked\n",
      "with interpreting multifactor personal data. This limitation restricts their\n",
      "applicability in domains that require context-aware decision-making tailored to\n",
      "individuals. This paper introduces Personalized Causal Graph Reasoning as an\n",
      "agentic framework that enhances LLM reasoning by incorporating personal causal\n",
      "graphs derived from data of individuals. These graphs provide a foundation that\n",
      "guides the LLM's reasoning process. We evaluate it on a case study on\n",
      "nutrient-oriented dietary recommendations, which requires personal reasoning\n",
      "due to the implicit unique dietary effects. We propose a counterfactual\n",
      "evaluation to estimate the efficiency of LLM-recommended foods for glucose\n",
      "management. Results demonstrate that the proposed method efficiently provides\n",
      "personalized dietary recommendations to reduce average glucose iAUC across\n",
      "three time windows, which outperforms the previous approach. LLM-as-a-judge\n",
      "evaluation results indicate that our proposed method enhances personalization\n",
      "in the reasoning process.\n",
      "--------------------------------------------------------------------------------\n",
      "ID: 204\n",
      "Title: Few-shot Personalization of LLMs with Mis-aligned Responses\n",
      "Similarity: 0.4272\n",
      "Summary: As the diversity of users increases, the capability of providing personalized\n",
      "responses by large language models (LLMs) has become increasingly important.\n",
      "Existing approaches have only limited successes in LLM personalization, due to\n",
      "the absence of personalized learning or the reliance on shared personal data.\n",
      "This paper proposes a new approach for a few-shot personalization of LLMs with\n",
      "their mis-aligned responses (Fermi). Our key idea is to learn a set of\n",
      "personalized prompts for each user by progressively improving the prompts using\n",
      "LLMs, based on user profile (e.g., demographic information) and a few examples\n",
      "of previous opinions. During an iterative process of prompt improvement, we\n",
      "incorporate the contexts of mis-aligned responses by LLMs, which are especially\n",
      "crucial for the effective personalization of LLMs. In addition, we develop an\n",
      "effective inference method to further leverage the context of the test query\n",
      "and the personalized prompts. Our experimental results demonstrate that Fermi\n",
      "significantly improves performance across various benchmarks, compared to\n",
      "best-performing baselines.\n",
      "--------------------------------------------------------------------------------\n",
      "ID: 453\n",
      "Title: Personalizing Content Moderation on Social Media: User Perspectives on Moderation Choices, Interface Design, and Labor\n",
      "Similarity: 0.4111\n",
      "Summary: Social media platforms moderate content for each user by incorporating the\n",
      "outputs of both platform-wide content moderation systems and, in some cases,\n",
      "user-configured personal moderation preferences. However, it is unclear (1) how\n",
      "end users perceive the choices and affordances of different kinds of personal\n",
      "content moderation tools, and (2) how the introduction of personalization\n",
      "impacts user perceptions of platforms' content moderation responsibilities.\n",
      "This paper investigates end users' perspectives on personal content moderation\n",
      "tools by conducting an interview study with a diverse sample of 24 active\n",
      "social media users. We probe interviewees' preferences using simulated personal\n",
      "moderation interfaces, including word filters, sliders for toxicity levels, and\n",
      "boolean toxicity toggles. We also examine the labor involved for users in\n",
      "choosing moderation settings and present users' attitudes about the roles and\n",
      "responsibilities of social media platforms and other stakeholders towards\n",
      "moderation. We discuss how our findings can inform design solutions to improve\n",
      "transparency and controllability in personal content moderation tools.\n",
      "--------------------------------------------------------------------------------\n",
      "ID: 125\n",
      "Title: Personalization of Large Language Models: A Survey\n",
      "Similarity: 0.4104\n",
      "Summary: Personalization of Large Language Models (LLMs) has recently become\n",
      "increasingly important with a wide range of applications. Despite the\n",
      "importance and recent progress, most existing works on personalized LLMs have\n",
      "focused either entirely on (a) personalized text generation or (b) leveraging\n",
      "LLMs for personalization-related downstream applications, such as\n",
      "recommendation systems. In this work, we bridge the gap between these two\n",
      "separate main directions for the first time by introducing a taxonomy for\n",
      "personalized LLM usage and summarizing the key differences and challenges. We\n",
      "provide a formalization of the foundations of personalized LLMs that\n",
      "consolidates and expands notions of personalization of LLMs, defining and\n",
      "discussing novel facets of personalization, usage, and desiderata of\n",
      "personalized LLMs. We then unify the literature across these diverse fields and\n",
      "usage scenarios by proposing systematic taxonomies for the granularity of\n",
      "personalization, personalization techniques, datasets, evaluation methods, and\n",
      "applications of personalized LLMs. Finally, we highlight challenges and\n",
      "important open problems that remain to be addressed. By unifying and surveying\n",
      "recent research using the proposed taxonomies, we aim to provide a clear guide\n",
      "to the existing literature and different facets of personalization in LLMs,\n",
      "empowering both researchers and practitioners.\n",
      "--------------------------------------------------------------------------------\n",
      "ID: 365\n",
      "Title: ChatDiet: Empowering Personalized Nutrition-Oriented Food Recommender Chatbots through an LLM-Augmented Framework\n",
      "Similarity: 0.3985\n",
      "Summary: The profound impact of food on health necessitates advanced\n",
      "nutrition-oriented food recommendation services. Conventional methods often\n",
      "lack the crucial elements of personalization, explainability, and\n",
      "interactivity. While Large Language Models (LLMs) bring interpretability and\n",
      "explainability, their standalone use falls short of achieving true\n",
      "personalization. In this paper, we introduce ChatDiet, a novel LLM-powered\n",
      "framework designed specifically for personalized nutrition-oriented food\n",
      "recommendation chatbots. ChatDiet integrates personal and population models,\n",
      "complemented by an orchestrator, to seamlessly retrieve and process pertinent\n",
      "information. The personal model leverages causal discovery and inference\n",
      "techniques to assess personalized nutritional effects for a specific user,\n",
      "whereas the population model provides generalized information on food\n",
      "nutritional content. The orchestrator retrieves, synergizes and delivers the\n",
      "output of both models to the LLM, providing tailored food recommendations\n",
      "designed to support targeted health outcomes. The result is a dynamic delivery\n",
      "of personalized and explainable food recommendations, tailored to individual\n",
      "user preferences. Our evaluation of ChatDiet includes a compelling case study,\n",
      "where we establish a causal personal model to estimate individual nutrition\n",
      "effects. Our assessments, including a food recommendation test showcasing a\n",
      "92\\% effectiveness rate, coupled with illustrative dialogue examples,\n",
      "underscore ChatDiet's strengths in explainability, personalization, and\n",
      "interactivity.\n",
      "--------------------------------------------------------------------------------\n",
      "ID: 493\n",
      "Title: Rehearse With User: Personalized Opinion Summarization via Role-Playing based on Large Language Models\n",
      "Similarity: 0.3957\n",
      "Summary: Personalized opinion summarization is crucial as it considers individual user\n",
      "interests while generating product summaries. Recent studies show that although\n",
      "large language models demonstrate powerful text summarization and evaluation\n",
      "capabilities without the need for training data, they face difficulties in\n",
      "personalized tasks involving long texts. To address this, \\textbf{Rehearsal}, a\n",
      "personalized opinion summarization framework via LLMs-based role-playing is\n",
      "proposed. Having the model act as the user, the model can better understand the\n",
      "user's personalized needs. Additionally, a role-playing supervisor and practice\n",
      "process are introduced to improve the role-playing ability of the LLMs, leading\n",
      "to a better expression of user needs. Furthermore, through suggestions from\n",
      "virtual users, the summary generation is intervened, ensuring that the\n",
      "generated summary includes information of interest to the user, thus achieving\n",
      "personalized summary generation. Experiment results demonstrate that our method\n",
      "can effectively improve the level of personalization in large model-generated\n",
      "summaries.\n",
      "--------------------------------------------------------------------------------\n",
      "ID: 326\n",
      "Title: Fair Personalization\n",
      "Similarity: 0.3953\n",
      "Summary: Personalization is pervasive in the online space as, when combined with\n",
      "learning, it leads to higher efficiency and revenue by allowing the most\n",
      "relevant content to be served to each user. However, recent studies suggest\n",
      "that such personalization can propagate societal or systemic biases, which has\n",
      "led to calls for regulatory mechanisms and algorithms to combat inequality.\n",
      "Here we propose a rigorous algorithmic framework that allows for the\n",
      "possibility to control biased or discriminatory personalization with respect to\n",
      "sensitive attributes of users without losing all of the benefits of\n",
      "personalization.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Connect to the SQLite database (or create it if it doesn't exist)\n",
    "conn = sqlite3.connect(\"arxiv_papers.db\")\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Load the sentence transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2') \n",
    "\n",
    "# NOTE: sanity check, the title of a paper\n",
    "query = \"Personalized Emphasis Framing for Persuasive Message\"\n",
    "\n",
    "#  Get the vector for the query\n",
    "query_embedding = model.encode([query])\n",
    "\n",
    "#  Fetch papers from the database\n",
    "cur.execute(\"SELECT id, title, summary FROM papers\")\n",
    "papers = cur.fetchall()\n",
    "\n",
    "#   Encode the summaries of the papers\n",
    "paper_embeddings = [model.encode([paper[2]]) for paper in papers]  # paper[2] is the summary\n",
    "\n",
    "#   Compute cosine similarities between the query and the paper summaries\n",
    "similarities = []\n",
    "for idx, paper_embedding in enumerate(paper_embeddings):\n",
    "    similarity = cosine_similarity(query_embedding, paper_embedding)\n",
    "    similarities.append((papers[idx], similarity[0][0]))  # (paper, similarity score)\n",
    "\n",
    "#  Sort papers by similarity \n",
    "similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "#   Print the most similar papers\n",
    "print(\"Most similar papers to your query:\")\n",
    "for paper, similarity in similarities[:10]:\n",
    "    print(f\"ID: {paper[0]}\")\n",
    "    print(f\"Title: {paper[1]}\")\n",
    "    print(f\"Similarity: {similarity:.4f}\")\n",
    "    print(f\"Summary: {paper[2]}\")\n",
    "    print('-' * 80)\n",
    "\n",
    "#   Close the connection to the database\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
