{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf4c9f8f",
   "metadata": {},
   "source": [
    "## TODO \n",
    "- maybe integrate the arxiv api and the similarity computation together (you can use the user query to use in the arxiv api)\n",
    "- somehow combine a chatbot with the retrieved papers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa94b86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marko\\miniconda3\\envs\\nlp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dcbae39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar papers to your query:\n",
      "ID: 77\n",
      "Title: Fine Grained Knowledge Transfer for Personalized Task-oriented Dialogue Systems\n",
      "Similarity: 0.4581\n",
      "Summary: Training a personalized dialogue system requires a lot of data, and the data\n",
      "collected for a single user is usually insufficient. One common practice for\n",
      "this problem is to share training dialogues between different users and train\n",
      "multiple sequence-to-sequence dialogue models together with transfer learning.\n",
      "However, current sequence-to-sequence transfer learning models operate on the\n",
      "entire sentence, which might cause negative transfer if different personal\n",
      "information from different users is mixed up. We propose a personalized decoder\n",
      "model to transfer finer granularity phrase-level knowledge between different\n",
      "users while keeping personal preferences of each user intact. A novel personal\n",
      "control gate is introduced, enabling the personalized decoder to switch between\n",
      "generating personalized phrases and shared phrases. The proposed personalized\n",
      "decoder model can be easily combined with various deep models and can be\n",
      "trained with reinforcement learning. Real-world experimental results\n",
      "demonstrate that the phrase-level personalized decoder improves the BLEU over\n",
      "multiple sentence-level transfer baseline models by as much as 7.5%.\n",
      "--------------------------------------------------------------------------------\n",
      "ID: 43\n",
      "Title: Person Search with Natural Language Description\n",
      "Similarity: 0.3815\n",
      "Summary: Searching persons in large-scale image databases with the query of natural\n",
      "language description has important applications in video surveillance. Existing\n",
      "methods mainly focused on searching persons with image-based or attribute-based\n",
      "queries, which have major limitations for a practical usage. In this paper, we\n",
      "study the problem of person search with natural language description. Given the\n",
      "textual description of a person, the algorithm of the person search is required\n",
      "to rank all the samples in the person database then retrieve the most relevant\n",
      "sample corresponding to the queried description. Since there is no person\n",
      "dataset or benchmark with textual description available, we collect a\n",
      "large-scale person description dataset with detailed natural language\n",
      "annotations and person samples from various sources, termed as CUHK Person\n",
      "Description Dataset (CUHK-PEDES). A wide range of possible models and baselines\n",
      "have been evaluated and compared on the person search benchmark. An Recurrent\n",
      "Neural Network with Gated Neural Attention mechanism (GNA-RNN) is proposed to\n",
      "establish the state-of-the art performance on person search.\n",
      "--------------------------------------------------------------------------------\n",
      "ID: 28\n",
      "Title: Transformer for Object Re-Identification: A Survey\n",
      "Similarity: 0.3799\n",
      "Summary: Object Re-identification (Re-ID) aims to identify specific objects across\n",
      "different times and scenes, which is a widely researched task in computer\n",
      "vision. For a prolonged period, this field has been predominantly driven by\n",
      "deep learning technology based on convolutional neural networks. In recent\n",
      "years, the emergence of Vision Transformers has spurred a growing number of\n",
      "studies delving deeper into Transformer-based Re-ID, continuously breaking\n",
      "performance records and witnessing significant progress in the Re-ID field.\n",
      "Offering a powerful, flexible, and unified solution, Transformers cater to a\n",
      "wide array of Re-ID tasks with unparalleled efficacy. This paper provides a\n",
      "comprehensive review and in-depth analysis of the Transformer-based Re-ID. In\n",
      "categorizing existing works into Image/Video-Based Re-ID, Re-ID with limited\n",
      "data/annotations, Cross-Modal Re-ID, and Special Re-ID Scenarios, we thoroughly\n",
      "elucidate the advantages demonstrated by the Transformer in addressing a\n",
      "multitude of challenges across these domains. Considering the trending\n",
      "unsupervised Re-ID, we propose a new Transformer baseline, UntransReID,\n",
      "achieving state-of-the-art performance on both single/cross modal tasks. For\n",
      "the under-explored animal Re-ID, we devise a standardized experimental\n",
      "benchmark and conduct extensive experiments to explore the applicability of\n",
      "Transformer for this task and facilitate future research. Finally, we discuss\n",
      "some important yet under-investigated open issues in the large foundation model\n",
      "era, we believe it will serve as a new handbook for researchers in this field.\n",
      "A periodically updated website will be available at\n",
      "https://github.com/mangye16/ReID-Survey.\n",
      "--------------------------------------------------------------------------------\n",
      "ID: 47\n",
      "Title: Sequential Transformer for End-to-End Person Search\n",
      "Similarity: 0.3714\n",
      "Summary: Person Search aims to simultaneously localize and recognize a target person\n",
      "from realistic and uncropped gallery images. One major challenge of person\n",
      "search comes from the contradictory goals of the two sub-tasks, i.e., person\n",
      "detection focuses on finding the commonness of all persons so as to distinguish\n",
      "persons from the background, while person re-identification (re-ID) focuses on\n",
      "the differences among different persons. In this paper, we propose a novel\n",
      "Sequential Transformer (SeqTR) for end-to-end person search to deal with this\n",
      "challenge. Our SeqTR contains a detection transformer and a novel re-ID\n",
      "transformer that sequentially addresses detection and re-ID tasks. The re-ID\n",
      "transformer comprises the self-attention layer that utilizes contextual\n",
      "information and the cross-attention layer that learns local fine-grained\n",
      "discriminative features of the human body. Moreover, the re-ID transformer is\n",
      "shared and supervised by multi-scale features to improve the robustness of\n",
      "learned person representations. Extensive experiments on two widely-used person\n",
      "search benchmarks, CUHK-SYSU and PRW, show that our proposed SeqTR not only\n",
      "outperforms all existing person search methods with a 59.3% mAP on PRW but also\n",
      "achieves comparable performance to the state-of-the-art results with an mAP of\n",
      "94.8% on CUHK-SYSU.\n",
      "--------------------------------------------------------------------------------\n",
      "ID: 25\n",
      "Title: Image Re-Identification: Where Self-supervision Meets Vision-Language Learning\n",
      "Similarity: 0.3668\n",
      "Summary: Recently, large-scale vision-language pre-trained models like CLIP have shown\n",
      "impressive performance in image re-identification (ReID). In this work, we\n",
      "explore whether self-supervision can aid in the use of CLIP for image ReID\n",
      "tasks. Specifically, we propose SVLL-ReID, the first attempt to integrate\n",
      "self-supervision and pre-trained CLIP via two training stages to facilitate the\n",
      "image ReID. We observe that: 1) incorporating language self-supervision in the\n",
      "first training stage can make the learnable text prompts more distinguishable,\n",
      "and 2) incorporating vision self-supervision in the second training stage can\n",
      "make the image features learned by the image encoder more discriminative. These\n",
      "observations imply that: 1) the text prompt learning in the first stage can\n",
      "benefit from the language self-supervision, and 2) the image feature learning\n",
      "in the second stage can benefit from the vision self-supervision. These\n",
      "benefits jointly facilitate the performance gain of the proposed SVLL-ReID. By\n",
      "conducting experiments on six image ReID benchmark datasets without any\n",
      "concrete text labels, we find that the proposed SVLL-ReID achieves the overall\n",
      "best performances compared with state-of-the-arts. Codes will be publicly\n",
      "available at https://github.com/BinWangGzhu/SVLL-ReID.\n",
      "--------------------------------------------------------------------------------\n",
      "ID: 34\n",
      "Title: MFP-VTON: Enhancing Mask-Free Person-to-Person Virtual Try-On via Diffusion Transformer\n",
      "Similarity: 0.3538\n",
      "Summary: The garment-to-person virtual try-on (VTON) task, which aims to generate\n",
      "fitting images of a person wearing a reference garment, has made significant\n",
      "strides. However, obtaining a standard garment is often more challenging than\n",
      "using the garment already worn by the person. To improve ease of use, we\n",
      "propose MFP-VTON, a Mask-Free framework for Person-to-Person VTON. Recognizing\n",
      "the scarcity of person-to-person data, we adapt a garment-to-person model and\n",
      "dataset to construct a specialized dataset for this task. Our approach builds\n",
      "upon a pretrained diffusion transformer, leveraging its strong generative\n",
      "capabilities. During mask-free model fine-tuning, we introduce a Focus\n",
      "Attention loss to emphasize the garment of the reference person and the details\n",
      "outside the garment of the target person. Experimental results demonstrate that\n",
      "our model excels in both person-to-person and garment-to-person VTON tasks,\n",
      "generating high-fidelity fitting images.\n",
      "--------------------------------------------------------------------------------\n",
      "ID: 23\n",
      "Title: Person Re-identification: Implicitly Defining the Receptive Fields of Deep Learning Classification Frameworks\n",
      "Similarity: 0.3383\n",
      "Summary: The \\emph{receptive fields} of deep learning classification models determine\n",
      "the regions of the input data that have the most significance for providing\n",
      "correct decisions. The primary way to learn such receptive fields is to train\n",
      "the models upon masked data, which helps the networks to ignore any unwanted\n",
      "regions, but has two major drawbacks: 1) it often yields edge-sensitive\n",
      "decision processes; and 2) augments the computational cost of the inference\n",
      "phase considerably. This paper describes a solution for implicitly driving the\n",
      "inference of the networks' receptive fields, by creating synthetic learning\n",
      "data composed of interchanged segments that should be \\emph{apriori}\n",
      "important/irrelevant for the network decision. In practice, we use a\n",
      "segmentation module to distinguish between the foreground\n",
      "(important)/background (irrelevant) parts of each learning instance, and\n",
      "randomly swap segments between image pairs, while keeping the class label\n",
      "exclusively consistent with the label of the deemed important segments. This\n",
      "strategy typically drives the networks to early convergence and appropriate\n",
      "solutions, where the identity and clutter descriptions are not correlated.\n",
      "Moreover, this data augmentation solution has various interesting properties:\n",
      "1) it is parameter-free; 2) it fully preserves the label information; and, 3)\n",
      "it is compatible with the typical data augmentation techniques. In the\n",
      "empirical validation, we considered the person re-identification problem and\n",
      "evaluated the effectiveness of the proposed solution in the well-known\n",
      "\\emph{Richly Annotated Pedestrian} (RAP) dataset for two different settings\n",
      "(\\emph{upper-body} and \\emph{full-body}), observing highly competitive results\n",
      "over the state-of-the-art. Under a reproducible research paradigm, both the\n",
      "code and the empirical evaluation protocol are available at\n",
      "\\url{https://github.com/Ehsan-Yaghoubi/reid-strong-baseline}.\n",
      "--------------------------------------------------------------------------------\n",
      "ID: 29\n",
      "Title: Siamese Networks for Cat Re-Identification: Exploring Neural Models for Cat Instance Recognition\n",
      "Similarity: 0.3313\n",
      "Summary: Street cats in urban areas often rely on human intervention for survival,\n",
      "leading to challenges in population control and welfare management. In April\n",
      "2023, Hello Inc., a Chinese urban mobility company, launched the Hello Street\n",
      "Cat initiative to address these issues. The project deployed over 21,000 smart\n",
      "feeding stations across 14 cities in China, integrating livestreaming cameras\n",
      "and treat dispensers activated through user donations. It also promotes the\n",
      "Trap-Neuter-Return (TNR) method, supported by a community-driven platform,\n",
      "HelloStreetCatWiki, where volunteers catalog and identify cats. However, manual\n",
      "identification is inefficient and unsustainable, creating a need for automated\n",
      "solutions. This study explores Deep Learning-based models for re-identifying\n",
      "street cats in the Hello Street Cat initiative. A dataset of 2,796 images of 69\n",
      "cats was used to train Siamese Networks with EfficientNetB0, MobileNet and\n",
      "VGG16 as base models, evaluated under contrastive and triplet loss functions.\n",
      "VGG16 paired with contrastive loss emerged as the most effective configuration,\n",
      "achieving up to 97% accuracy and an F1 score of 0.9344 during testing. The\n",
      "approach leverages image augmentation and dataset refinement to overcome\n",
      "challenges posed by limited data and diverse visual variations. These findings\n",
      "underscore the potential of automated cat re-identification to streamline\n",
      "population monitoring and welfare efforts. By reducing reliance on manual\n",
      "processes, the method offers a scalable and reliable solution for\n",
      "communitydriven initiatives. Future research will focus on expanding datasets\n",
      "and developing real-time implementations to enhance practicality in large-scale\n",
      "deployments.\n",
      "--------------------------------------------------------------------------------\n",
      "ID: 13\n",
      "Title: Deep Recurrent Convolutional Networks for Video-based Person Re-identification: An End-to-End Approach\n",
      "Similarity: 0.3304\n",
      "Summary: In this paper, we present an end-to-end approach to simultaneously learn\n",
      "spatio-temporal features and corresponding similarity metric for video-based\n",
      "person re-identification. Given the video sequence of a person, features from\n",
      "each frame that are extracted from all levels of a deep convolutional network\n",
      "can preserve a higher spatial resolution from which we can model finer motion\n",
      "patterns. These low-level visual percepts are leveraged into a variant of\n",
      "recurrent model to characterize the temporal variation between time-steps.\n",
      "Features from all time-steps are then summarized using temporal pooling to\n",
      "produce an overall feature representation for the complete sequence. The deep\n",
      "convolutional network, recurrent layer, and the temporal pooling are jointly\n",
      "trained to extract comparable hidden-unit representations from input pair of\n",
      "time series to compute their corresponding similarity value. The proposed\n",
      "framework combines time series modeling and metric learning to jointly learn\n",
      "relevant features and a good similarity measure between time sequences of\n",
      "person.\n",
      "  Experiments demonstrate that our approach achieves the state-of-the-art\n",
      "performance for video-based person re-identification on iLIDS-VID and PRID\n",
      "2011, the two primary public datasets for this purpose.\n",
      "--------------------------------------------------------------------------------\n",
      "ID: 73\n",
      "Title: Prototype-Guided Text-based Person Search based on Rich Chinese Descriptions\n",
      "Similarity: 0.3146\n",
      "Summary: Text-based person search aims to simultaneously localize and identify the\n",
      "target person based on query text from uncropped scene images, which can be\n",
      "regarded as the unified task of person detection and text-based person\n",
      "retrieval task. In this work, we propose a large-scale benchmark dataset named\n",
      "PRW-TPS-CN based on the widely used person search dataset PRW. Our dataset\n",
      "contains 47,102 sentences, which means there is quite more information than\n",
      "existing dataset. These texts precisely describe the person images from top to\n",
      "bottom, which in line with the natural description order. We also provide both\n",
      "Chinese and English descriptions in our dataset for more comprehensive\n",
      "evaluation. These characteristics make our dataset more applicable. To\n",
      "alleviate the inconsistency between person detection and text-based person\n",
      "retrieval, we take advantage of the rich texts in PRW-TPS-CN dataset. We\n",
      "propose to aggregate multiple texts as text prototypes to maintain the\n",
      "prominent text features of a person, which can better reflect the whole\n",
      "character of a person. The overall prototypes lead to generating the image\n",
      "attention map to eliminate the detection misalignment causing the decrease of\n",
      "text-based person retrieval. Thus, the inconsistency between person detection\n",
      "and text-based person retrieval is largely alleviated. We conduct extensive\n",
      "experiments on the PRW-TPS-CN dataset. The experimental results show the\n",
      "PRW-TPS-CN dataset's effectiveness and the state-of-the-art performance of our\n",
      "approach.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Connect to the SQLite database (or create it if it doesn't exist)\n",
    "conn = sqlite3.connect(\"arxiv_papers.db\")\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Load the sentence transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2') \n",
    "\n",
    "# NOTE: sanity check, a sentence from a papers abstract\n",
    "query = \"The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer,\"\n",
    "\n",
    "#  Get the vector for the query\n",
    "query_embedding = model.encode([query])\n",
    "\n",
    "#  Fetch papers from the database\n",
    "cur.execute(\"SELECT id, title, summary FROM papers\")\n",
    "papers = cur.fetchall()\n",
    "\n",
    "#   Encode the summaries of the papers\n",
    "paper_embeddings = [model.encode([paper[2]]) for paper in papers]  # paper[2] is the summary\n",
    "\n",
    "#   Compute cosine similarities between the query and the paper summaries\n",
    "similarities = []\n",
    "for idx, paper_embedding in enumerate(paper_embeddings):\n",
    "    similarity = cosine_similarity(query_embedding, paper_embedding)\n",
    "    similarities.append((papers[idx], similarity[0][0]))  # (paper, similarity score)\n",
    "\n",
    "#  Sort papers by similarity \n",
    "similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "#   Print the most similar papers\n",
    "print(\"Most similar papers to your query:\")\n",
    "for paper, similarity in similarities[:10]:\n",
    "    print(f\"ID: {paper[0]}\")\n",
    "    print(f\"Title: {paper[1]}\")\n",
    "    print(f\"Similarity: {similarity:.4f}\")\n",
    "    print(f\"Summary: {paper[2]}\")\n",
    "    print('-' * 80)\n",
    "\n",
    "#   Close the connection to the database\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
