%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% FRI Data Science_report LaTeX Template
% Version 1.0 (28/1/2020)
% 
% Jure Demšar (jure.demsar@fri.uni-lj.si)
%
% Based on MicromouseSymp article template by:
% Mathias Legrand (legrand.mathias@gmail.com) 
% With extensive modifications by:
% Antonio Valente (antonio.luis.valente@gmail.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------
\documentclass[fleqn,moreauthors,10pt]{ds_report}
\usepackage[english]{babel}

\graphicspath{{fig/}}




%----------------------------------------------------------------------------------------
%	ARTICLE INFORMATION
%----------------------------------------------------------------------------------------

% Header
\JournalInfo{FRI Natural language processing course 2025}

% Interim or final report
\Archive{Project report} 
%\Archive{Final report} 

% Article title
\PaperTitle{Conversational Agent with Retrieval-Augmented Generation for research assistance} 

% Authors (student competitors) and their info
\Authors{Marko Medved, Sebastijan Trojer, and Matej Vrečar}

% Advisors
\affiliation{\textit{Advisor: Aleš Žagar}}

% Keywords
\Keywords{Retrieval augmented generation, research assistance, re-identification}
\newcommand{\keywordname}{Keywords}


%----------------------------------------------------------------------------------------
%	ABSTRACT
%----------------------------------------------------------------------------------------

\Abstract{
Large language models (LLMs) are increasingly used in conversational agents for academic support, but they often struggle with outdated knowledge and hallucinations in specialized domains. To address this, we developed a conversational assistant using Retrieval-Augmented Generation (RAG), focusing on the field of automatic re-identification. Our system retrieves recent research papers from arXiv and CVF, processes them using domain-adapted query rewriting and relevance-based re-ranking, and integrates this context into language model responses. We implemented and evaluated both a baseline and an attempt of
 an improved retrieval pipeline, emphasizing relevance over exact matches. 
% TODO results
}

%----------------------------------------------------------------------------------------

\begin{document}

% Makes all text pages the same height
\flushbottom 

% Print the title and abstract box
\maketitle 

% Removes page numbering from the first page
\thispagestyle{empty} 

%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------

\section*{Introduction}
LLMs have become increasingly capable in natural language understanding and generation, making them attractive for use in academic research support. However, their utility is limited by knowledge cutoffs and susceptibility to hallucinations, particularly in fast-evolving or specialized fields. To address these limitations, we developed a conversational assistant based on RAG, targeting the domain of person re-identification—a challenging task in computer vision.

Our system combines a pretrained language model with a retrieval pipeline that fetches recent research papers from sources like arXiv and CVF. The goal is to provide accurate, up-to-date answers by integrating external knowledge into the model's responses. In this project, we emphasize improvements to the retrieval process, which is critical for maximizing the relevance and utility of generated responses, by providing relevant context.

We implemented both a baseline and an enhanced retrieval pipeline.
 The latter includes query rewriting, domain-specific filtering, 
 and re-ranking strategies aimed at improving contextual relevance.
  Evaluation focuses on the system's ability to retrieve relevant papers and generate
   accurate responses to academic queries, particularly those related to recent developments 
   in automatic re-identification. We compare the performance of our baseline RAG system 
   with the enhanced pipeline, assessing retrieval effectiveness using 
   metrics like MRR, nDCG and CMC.




%------------------------------------------------

\section*{Related work}

% Joined

Large Language Models have demonstrated strong language understanding and generation capabilities, but they often struggle with hallucinations, outdated knowledge, and limited domain-specific accuracy, especially when handling specialized research queries~\cite{kandpal2023longtail}. Retrieval-Augmented Generation (RAG) addresses these challenges by enhancing LLMs with real-time information retrieval, improving factual accuracy and relevance~\cite{gao2024rag}. Early RAG models followed a basic retrieve-then-generate structure, but more advanced approaches have introduced techniques like query rewriting, reranking, and adaptive retrieval to improve context relevance and coherence~\cite{gao2024rag}.

Recent research has shown the effectiveness of RAG-based systems for academic and domain-specific applications. For example,~\cite{Chatbots_in_Academia} implemented a RAG system using OpenAI Ada embeddings and GPT-3.5 to enhance information retrieval and synthesis for academic documents, demonstrating the importance of precise embedding strategies for research-based chatbots. Afzal et al.~\cite{afzal2024towards} focused on domain-specific data by creating the CurriculumQA dataset and optimizing retrieval with tailored evaluation metrics such as relevance, coherence, and faithfulness, highlighting the importance of fine-tuning for specific academic fields. Similarly, Yasin et al.~\cite{yasin2024retrieval} integrated advanced techniques—including fine-tuned embedding models, semantic chunking, and abstract-first retrieval—to improve the accuracy and relevance of scholarly content retrieval.

Evaluating RAG systems remains complex due to the nuanced nature of retrieval and generation quality. Chen et al.~\cite{chen2024benchmarking} introduced the Retrieval-Augmented Generation Benchmark (RGB) to measure RAG performance across four key dimensions: noise robustness, negative rejection, information integration, and counterfactual robustness. Their findings show that while RAG improves LLM accuracy, it still struggles with integrating long-distance information and handling noisy or uncertain evidence. Common evaluation methods for RAG systems include retrieval-focused metrics like MRR (Mean Reciprocal Rank) and nDCG (normalized Discounted Cumulative Gain), as well as generation-focused metrics like BLEU and ROUGE to measure the quality and relevance of generated responses~\cite{lewis2020rag}.

In our project, we aim to address these issues by developing a RAG-based chatbot for academic research in automatic re-identification—the task of matching individuals across different cameras or settings, a complex problem in computer vision and machine learning. By focusing on recent research papers from arXiv, we seek to enhance the chatbot’s ability to retrieve and generate accurate responses about the latest developments in automatic re-identification


\section*{Methods}
\subsection*{Baseline}
\subsubsection*{Paper Retrieval from arXiv}
We begin by retrieving candidate papers using the official arXiv API. For each paper, we extract its title, abstract, and metadata. To enable semantic similarity-based filtering, both the user query and the abstracts are embedded using the \textit{allenai-specter} SentenceTransformer model, which is trained to map scientific documents into a dense vector space. We compute cosine similarity between the query embedding and each abstract embedding, and select the top-$k$ most 
relevant papers (In the main evaluation in this project we use $k=30$).

\subsubsection*{Context Construction}
The titles and abstracts of the top-ranked papers are concatenated into a single text block to serve as external context. This context is inserted into the prompt presented to the language model. No chunking, reranking, or content selection is applied at this stage beyond the similarity-based retrieval.

\subsubsection*{Language Model Inference}
We use \textit{Mistral-7B-Instruct-v0.2} as the language model backend. To enable efficient inference on commodity hardware, the model is loaded using 4-bit quantization with \textit{bitsandbytes}, applying NF4 quantization and bfloat16 compute precision. The model is wrapped with Hugging Face's text-generation pipeline and integrated with LangChain to enable prompt-based question answering. Prompt templates include both the constructed context and the current user query.
 For comparison, we also evaluate answers generated without any retrieved context in the preliminary results.

\subsubsection*{System Behavior}
The system retains a basic conversational memory by concatenating the most recent user query with the current one. This helps preserve short-term context without implementing a full dialogue memory or rewriting mechanism. No fine-tuning is performed, and no re-ranking, filtering, or external knowledge base is used. This baseline is intended to evaluate performance with minimal architectural complexity.


\subsection*{Attempt at an Improved pipeline for retrieval}

A modified RAG system was implemented to explore potential improvements in relevance and accuracy for person re-identification from video. The updated pipeline introduces several architectural changes aimed at more targeted retrieval and domain-aligned generation:

\begin{itemize}
    \item \textit{Two-stage query rewriting}: Queries are rewritten separately for retrieval (search) and semantic matching, allowing distinct optimization for recall and ranking.

    \item \textit{Domain-specific augmentation}: Rewritten queries are enriched with terms such as \textit{person re-identification} and restricted to the \textit{cs.CV} category to better focus the search on relevant literature.

    \item \textit{Local database integration}: A local CVF paper database supplements 
    arXiv results, expanding the overall coverage of relevant academic sources. To generate this 
    local database we designed a web-scraper that scraped the papers from recent most influential 
    computer vision conferences(CVPR, ICCV, ECCV).

    \item \textit{Task-specific prompting}: Prompts used during response generation are tailored for the Re-ID task to better align outputs with domain-specific needs.
\end{itemize}

Figure~\ref{fig:retrieval_pipeline} presents an overview of the enhanced 
pipeline. These modifications were 
 evaluated against the baseline architecture, with comparative analysis 
 focusing on retrieval relevance.


\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{fig/flowchart.pdf}
    \caption{Overview of the enhanced retrieval pipeline.}
    \label{fig:retrieval_pipeline}
\end{figure*}







\section*{Preliminary results }
We evaluated our RAG-enhanced approach on scientific questions targeting papers published after December 2023 to test its ability to incorporate recent, unseen research.

While top-$k=5$ retrieval rarely surfaced the exact target paper, responses were often more informative and current due to related recent articles. Increasing to $k=10$ improved retrieval in several cases—for example, retrieving a near-duplicate sentiment analysis paper by the same authors.

Query specificity significantly improved performance. \\Adding terms like “low contamination regime” and “near-linear sample complexity” helped retrieve the correct covariance estimator paper. Generic queries, by contrast, often led to irrelevant domains (e.g., quantum mechanics).

For complex topics like NP-complete problems and object detection, specific queries led to responses better aligned with recent research trends. However, the system occasionally generated sub-questions, increasing inference time.

These results suggest RAG improves relevance and recency, especially with larger $k$ and well-formed queries, though retrieval control remains a challenge. Full examples and logs are available in results/preliminary\_tests.docx.

\section*{Results}
\subsection*{Evaluation Description}

As part of the evaluation protocol, we simulate the retrieval task by selecting a specific target paper
and generating two types of queries per paper:

\begin{itemize}
    \item \textit{Specific} query: designed to closely reflect the paper’s title or main content.
    \item \textit{Broad} query: incorporates more general or related terms to capture a wider set of relevant documents.
\end{itemize}

The queries and paper titles are available in the \textit{results/queries.txt} file.

This setup allows us to assess the retrieval system’s ability to rank truly relevant papers near the top of the search results,
which simulates real-world scenarios where researchers look for both precise and comprehensive literature coverage.

We search for 100 papers with the arxiv module and then choose the top 30 most similar 
papers to use in our evalution. 

To ensure a fair comparison with the baseline system, we did not include our local CVF paper database during evaluation,
as the baseline does not have access to it.

\subsubsection*{Metrics for Retrieval Evaluation}

To evaluate retrieval performance, we consider both early precision and full-ranking quality:

\begin{itemize}
    \item \textbf{Mean Reciprocal Rank (MRR)} measures the inverse of the rank of the first relevant result for each query. 
    It emphasizes how early the first correct result appears in the list.
    
    \item \textbf{Normalized Discounted Cumulative Gain (nDCG)} quantifies the usefulness of retrieved results based on their rank, 
    with gains discounted logarithmically. It accounts for the position and relevance of multiple relevant documents in the list, 
    normalized against the ideal DCG for the same set.
\end{itemize}

Additionally, to visualize the retrieval quality across different rank cutoffs, we compute and plot the Cumulative Matching Characteristic (CMC) curves.
These curves show the probability that a correct match appears within the top-$k$ retrieved results and give insight into top-k retrieval behavior.

\subsection*{Evaluation Results}

To estimate uncertainty in our evaluation, we used non-parametric bootstrapping over the query set with 1000 resamples. 
For each resample, we computed both the CMC curves and the MRR and nDCG scores.
The values reported in Table~1 represent the mean and standard deviation across these bootstrap samples.

For the CMC plots (Figure~\ref{fig:cmc}), we show the mean curve along with confidence intervals.
These intervals represent the $[15^\text{th}, 85^\text{th}]$ percentiles (i.e., a 70\% confidence interval)
computed at each rank position based on bootstrapped CMC values.

As seen in Table~1 and Figure~\ref{fig:cmc}, our current method underperforms the baseline across all metrics,
with the largest gap observed in the specific query setting.
This indicates that the current approach may not be capturing the essential characteristics needed or that the enhanced queries are not suitable for the embedding model that we use.

Improving performance may require exploring different models or techniques that better represent
the relationship between queries and target papers, particularly in more precise retrieval scenarios.

\begin{figure}[h]
    \centering
    \includegraphics[width=\columnwidth]{fig/cmc.png}
    \caption{CMC curve comparison}
    \label{fig:cmc}
\end{figure}

\begin{table}[h]
\centering
\caption{Retrieval performance for specific and broad queries}
\begin{tabular}{l|c|c}
\toprule
\multicolumn{3}{c}{\textbf{Specific Queries}} \\
\midrule
\textbf{Method} & \textbf{MRR} & \textbf{nDCG} \\
\midrule
Current  & 0.361 ± 0.096 & 1.155 ± 0.272 \\
Baseline & \textbf{0.683 ± 0.090} & \textbf{2.056 ± 0.237} \\
\midrule[0.8pt]
\multicolumn{3}{c}{\textbf{Broad Queries}} \\
\midrule
\textbf{Method} & \textbf{MRR} & \textbf{nDCG} \\
\midrule
Current  & 0.236 ± 0.085 & 0.778 ± 0.244 \\
Baseline & \textbf{0.369 ± 0.078} & \textbf{1.309 ± 0.214} \\
\bottomrule
\end{tabular}
\end{table}

Note that you can access the full rankings of all of of the papers in \textit{results/}



%------------------------------------------------


%------------------------------------------------

\section*{Discussion}
% evalvacija težka, ker delamo na unabeled data, in je treba ročno preverjati rezultate
While the system showed promise in generating more informed responses, it also highlighted several challenges and areas for improvement.
One of the main challenges we faced was the difficulty of evaluating retrieval performance on unlabeled data. Since our queries were designed to retrieve specific papers, we had to manually verify the relevance of retrieved results, which is time-consuming and subjective. Moreover, subjectivity was also present in query writing itself, and since we required two queries per article, it was difficult to keep the two independent from one another. This limitation shows the need for better evaluation protocols in RAG systems, particularly when dealing with specialized domains where labeled datasets may not be readily available.

Furthermore, while our enhanced retrieval pipeline utilized query rewriting and re-ranking strategies, it still underperformed. The issue might stem from the fact that the embedding model was trained on data, that resembled the raw abstracts, rather than the rewritten queries we used. This suggests that further fine-tuning of the embedding model, or using different model altogether could be more suitable for our use-case, while the current model can still serve as a baseline.

Moreover, another possible approach would be not to only look at man-made abstracts, but also use a summarization LLM, to draw the important information from the entire paper.

\section*{Conclusion}
In this project, we developed a conversational agent using Retrieval-Augmented Generation (RAG) to assist with academic research in automatic re-identification. Our approach aimed to enhance the language model's responses by integrating recent research papers from arXiv and CVF, focusing on improving retrieval relevance and accuracy. 
While our modified RAG approach did not produce results that were better than the baseline, it provided meaningful insights into its drabwacks, as well as opened further research questions.


%----------------------------------------------------------------------------------------
%	REFERENCE LIST
%----------------------------------------------------------------------------------------
\bibliographystyle{unsrt}
\bibliography{report}


\end{document}
