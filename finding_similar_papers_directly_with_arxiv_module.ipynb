{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf4c9f8f",
   "metadata": {},
   "source": [
    "## TODO \n",
    "- maybe integrate the arxiv api and the similarity computation together (you can use the user query to use in the arxiv api)\n",
    "- somehow combine a chatbot with the retrieved papers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa94b86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marko\\miniconda3\\envs\\nlp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dcbae39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marko\\AppData\\Local\\Temp\\ipykernel_18896\\2693033379.py:17: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  results = list(search.results())  # Convert generator to list\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank #1\n",
      "Title: Two-Headed Monster And Crossed Co-Attention Networks\n",
      "Authors: Yaoyiran Li, Jing Jiang\n",
      "Summary: This paper presents some preliminary investigations of a new co-attention\n",
      "mechanism in neural transduction models. We propose a paradigm, termed\n",
      "Two-Headed Monster (THM), which consists of two symmetric encoder modules and\n",
      "one decoder module connected with co-attention. As a specific and concrete\n",
      "implementation of THM, Crossed Co-Attention Networks (CCNs) are designed based\n",
      "on the Transformer model. We demonstrate CCNs on WMT 2014 EN-DE and WMT 2016\n",
      "EN-FI translation tasks and our model outperforms the strong Transformer\n",
      "baseline by 0.51 (big) and 0.74 (base) BLEU points on EN-DE and by 0.17 (big)\n",
      "and 0.47 (base) BLEU points on EN-FI.\n",
      "Similarity: 0.7980\n",
      "URL: https://arxiv.org/abs/1911.03897v1\n",
      "--------------------------------------------------------------------------------\n",
      "Rank #2\n",
      "Title: Understanding How Encoder-Decoder Architectures Attend\n",
      "Authors: Kyle Aitken, Vinay V Ramasesh, Yuan Cao, Niru Maheswaranathan\n",
      "Summary: Encoder-decoder networks with attention have proven to be a powerful way to\n",
      "solve many sequence-to-sequence tasks. In these networks, attention aligns\n",
      "encoder and decoder states and is often used for visualizing network behavior.\n",
      "However, the mechanisms used by networks to generate appropriate attention\n",
      "matrices are still mysterious. Moreover, how these mechanisms vary depending on\n",
      "the particular architecture used for the encoder and decoder (recurrent,\n",
      "feed-forward, etc.) are also not well understood. In this work, we investigate\n",
      "how encoder-decoder networks solve different sequence-to-sequence tasks. We\n",
      "introduce a way of decomposing hidden states over a sequence into temporal\n",
      "(independent of input) and input-driven (independent of sequence position)\n",
      "components. This reveals how attention matrices are formed: depending on the\n",
      "task requirements, networks rely more heavily on either the temporal or\n",
      "input-driven components. These findings hold across both recurrent and\n",
      "feed-forward architectures despite their differences in forming the temporal\n",
      "components. Overall, our results provide new insight into the inner workings of\n",
      "attention-based encoder-decoder networks.\n",
      "Similarity: 0.7192\n",
      "URL: https://arxiv.org/abs/2110.15253v1\n",
      "--------------------------------------------------------------------------------\n",
      "Rank #3\n",
      "Title: Multiresolution Transformer Networks: Recurrence is Not Essential for Modeling Hierarchical Structure\n",
      "Authors: Vikas K. Garg, Inderjit S. Dhillon, Hsiang-Fu Yu\n",
      "Summary: The architecture of Transformer is based entirely on self-attention, and has\n",
      "been shown to outperform models that employ recurrence on sequence transduction\n",
      "tasks such as machine translation. The superior performance of Transformer has\n",
      "been attributed to propagating signals over shorter distances, between\n",
      "positions in the input and the output, compared to the recurrent architectures.\n",
      "We establish connections between the dynamics in Transformer and recurrent\n",
      "networks to argue that several factors including gradient flow along an\n",
      "ensemble of multiple weakly dependent paths play a paramount role in the\n",
      "success of Transformer. We then leverage the dynamics to introduce {\\em\n",
      "Multiresolution Transformer Networks} as the first architecture that exploits\n",
      "hierarchical structure in data via self-attention. Our models significantly\n",
      "outperform state-of-the-art recurrent and hierarchical recurrent models on two\n",
      "real-world datasets for query suggestion, namely, \\aol and \\amazon. In\n",
      "particular, on AOL data, our model registers at least 20\\% improvement on each\n",
      "precision score, and over 25\\% improvement on the BLEU score with respect to\n",
      "the best performing recurrent model. We thus provide strong evidence that\n",
      "recurrence is not essential for modeling hierarchical structure.\n",
      "Similarity: 0.7139\n",
      "URL: https://arxiv.org/abs/1908.10408v1\n",
      "--------------------------------------------------------------------------------\n",
      "Rank #4\n",
      "Title: Local Monotonic Attention Mechanism for End-to-End Speech and Language Processing\n",
      "Authors: Andros Tjandra, Sakriani Sakti, Satoshi Nakamura\n",
      "Summary: Recently, encoder-decoder neural networks have shown impressive performance\n",
      "on many sequence-related tasks. The architecture commonly uses an attentional\n",
      "mechanism which allows the model to learn alignments between the source and the\n",
      "target sequence. Most attentional mechanisms used today is based on a global\n",
      "attention property which requires a computation of a weighted summarization of\n",
      "the whole input sequence generated by encoder states. However, it is\n",
      "computationally expensive and often produces misalignment on the longer input\n",
      "sequence. Furthermore, it does not fit with monotonous or left-to-right nature\n",
      "in several tasks, such as automatic speech recognition (ASR),\n",
      "grapheme-to-phoneme (G2P), etc. In this paper, we propose a novel attention\n",
      "mechanism that has local and monotonic properties. Various ways to control\n",
      "those properties are also explored. Experimental results on ASR, G2P and\n",
      "machine translation between two languages with similar sentence structures,\n",
      "demonstrate that the proposed encoder-decoder model with local monotonic\n",
      "attention could achieve significant performance improvements and reduce the\n",
      "computational complexity in comparison with the one that used the standard\n",
      "global attention architecture.\n",
      "Similarity: 0.6995\n",
      "URL: https://arxiv.org/abs/1705.08091v2\n",
      "--------------------------------------------------------------------------------\n",
      "Rank #5\n",
      "Title: Attention Is All You Need\n",
      "Authors: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\n",
      "Summary: The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks in an encoder-decoder configuration. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer, based\n",
      "solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine translation tasks show these models to be\n",
      "superior in quality while being more parallelizable and requiring significantly\n",
      "less time to train. Our model achieves 28.4 BLEU on the WMT 2014\n",
      "English-to-German translation task, improving over the existing best results,\n",
      "including ensembles by over 2 BLEU. On the WMT 2014 English-to-French\n",
      "translation task, our model establishes a new single-model state-of-the-art\n",
      "BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\n",
      "of the training costs of the best models from the literature. We show that the\n",
      "Transformer generalizes well to other tasks by applying it successfully to\n",
      "English constituency parsing both with large and limited training data.\n",
      "Similarity: 0.6878\n",
      "URL: https://arxiv.org/abs/1706.03762v7\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Load the sentence transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2') \n",
    "\n",
    "# Define your query\n",
    "user_query = \"This paper presents some preliminary investigations of a new co-attention, the dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer,\"\n",
    "\n",
    "# Get the embedding for the query\n",
    "query_embedding = model.encode([user_query])\n",
    "\n",
    "# Use arxiv to search for papers (limit to 100 results)\n",
    "search = arxiv.Search(\n",
    "    query=user_query,\n",
    "    max_results=50,\n",
    "    sort_by=arxiv.SortCriterion.Relevance,\n",
    "    sort_order=arxiv.SortOrder.Descending\n",
    "      )\n",
    "results = list(search.results())  # Convert generator to list\n",
    "\n",
    "# Extract summaries and titles\n",
    "papers = []\n",
    "summaries = []\n",
    "for result in results:\n",
    "    title = result.title\n",
    "    authors = ', '.join([author.name for author in result.authors])\n",
    "    summary = result.summary\n",
    "    url = f\"https://arxiv.org/abs/{result.entry_id.split('/')[-1]}\"\n",
    "    papers.append({\n",
    "        \"title\": title,\n",
    "        \"authors\": authors,\n",
    "        \"summary\": summary,\n",
    "        \"url\": url\n",
    "    })\n",
    "    summaries.append(summary)\n",
    "\n",
    "# Encode all summaries\n",
    "summary_embeddings = model.encode(summaries)\n",
    "\n",
    "# Compute cosine similarities\n",
    "similarities = cosine_similarity(query_embedding, summary_embeddings)[0]\n",
    "\n",
    "# Attach similarity scores to papers and sort\n",
    "for i, paper in enumerate(papers):\n",
    "    paper[\"similarity\"] = similarities[i]\n",
    "\n",
    "top_papers = sorted(papers, key=lambda x: x[\"similarity\"], reverse=True)[:5]\n",
    "\n",
    "# Print top 5 similar papers\n",
    "for i, paper in enumerate(top_papers, 1):\n",
    "    print(f\"Rank #{i}\")\n",
    "    print(f\"Title: {paper['title']}\")\n",
    "    print(f\"Authors: {paper['authors']}\")\n",
    "    print(f\"Summary: {paper['summary']}\")\n",
    "    print(f\"Similarity: {paper['similarity']:.4f}\")\n",
    "    print(f\"URL: {paper['url']}\")\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdc3efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# Load a chat-capable model\n",
    "model_id = \"HuggingFaceH4/zephyr-7b-beta\"  # You can replace with another chat model if you want\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "# Setup chat pipeline\n",
    "rag = pipeline(\"chat\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Combine summaries into a context string\n",
    "context = \"\\n\\n\".join(\n",
    "    f\"Title: {paper['title']}\\nSummary: {paper['summary']}\" for paper in top_papers\n",
    ")\n",
    "\n",
    "# Prepare system + user messages\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI that answers user questions based on provided research papers.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"\"\"Here are some research papers:\n",
    "\n",
    "{context}\n",
    "\n",
    "Use these summaries to answer the following research question (also cite the papers):\n",
    "\n",
    "Question: {user_query}\n",
    "Answer:\"\"\"}\n",
    "]\n",
    "\n",
    "# Generate the answer\n",
    "output = rag(messages, max_new_tokens=300)\n",
    "\n",
    "# Print results\n",
    "print(\"Research Papers and Generated Answer:\")\n",
    "print(f\"Research Papers:\\n{context}\")  # Full context\n",
    "print(f\"Generated Answer:\\n{output[0]['generated_text']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c9bbaa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1280 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Research Papers and Generated Answer:\n",
      "Research Papers:\n",
      "Title: Two-Headed Monster And Crossed Co-Attention Networks\n",
      "Summary: This paper presents some preliminary investigations of a new co-attention\n",
      "mechanism in neural transduction models. We propose a paradigm, termed\n",
      "Two-Headed Monster (THM), which consists of two symmetric encoder modules and\n",
      "one decoder module connected with co-attention. As a specific and concrete\n",
      "implementation of THM, Crossed Co-Attention Networks (CCNs) are designed based\n",
      "on the Transformer model. We demonstrate CCNs on WMT 2014 EN-DE and WMT 2016\n",
      "EN-FI translation tasks and our model outperforms the strong Transformer\n",
      "baseline by 0.51 (big) and 0.74 (base) BLEU points on EN-DE and by 0.17 (big)\n",
      "and 0.47 (base) BLEU points on EN-FI.\n",
      "\n",
      "Title: Understanding How Encoder-Decoder Architectures Attend\n",
      "Summary: Encoder-decoder networks with attention have proven to be a powerful way to\n",
      "solve many sequence-to-sequence tasks. In these networks, attention aligns\n",
      "encoder and decoder states and is often used for visualizing network behavior.\n",
      "However, the mechanisms used by networks to generate appropriate attention\n",
      "matrices are still mysterious. Moreover, how these mechanisms vary depending on\n",
      "the particular architecture used for the encoder and decoder (recurrent,\n",
      "feed-forward, etc.) are also not well understood. In this work, we investigate\n",
      "how encoder-decoder networks solve different sequence-to-sequence tasks. We\n",
      "introduce a way of decomposing hidden states over a sequence into temporal\n",
      "(independent of input) and input-driven (independent of sequence position)\n",
      "components. This reveals how attention matrices are formed: depending on the\n",
      "task requirements, networks rely more heavily on either the temporal or\n",
      "inpu\n",
      "Generated Answer:\n",
      "A new architecture for co-attention in neural transduction models\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "\n",
    "model_id = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
    "\n",
    "rag = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Combine summaries into a context string, but make sure it's within the token limit\n",
    "context = \"\\n\\n\".join(\n",
    "    f\"Title: {paper['title']}\\nSummary: {paper['summary']}\" for paper in top_papers\n",
    ")\n",
    "\n",
    "# Encode the context and check its length\n",
    "input_ids = tokenizer.encode(context, return_tensors=\"pt\")\n",
    "max_length = 1700  # Adjust this based on your model's max token length\n",
    "\n",
    "# Truncate if necessary to fit within the max token limit\n",
    "if input_ids.shape[1] > max_length:\n",
    "    input_ids = input_ids[:, :max_length]\n",
    "\n",
    "\n",
    "# Prepare the prompt, ensuring it stays within the token limit\n",
    "prompt = f\"\"\"Here are some research papers:\n",
    "\n",
    "{context[:max_length]}  # Only include a truncated context if necessary\n",
    "\n",
    "Use the above research paper summaries to answer the following question:\n",
    "\n",
    "Question: {user_query}\n",
    "Answer:\"\"\"\n",
    "\n",
    "# Generate the answer using the same prompt\n",
    "output = rag(prompt, max_new_tokens=300)\n",
    "\n",
    "# Provide the generated answer along with the papers\n",
    "print(\"Research Papers and Generated Answer:\")\n",
    "print(f\"Research Papers:\\n{context[:max_length]}\")  # Display truncated context\n",
    "print(f\"Generated Answer:\\n{output[0]['generated_text']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
