{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf4c9f8f",
   "metadata": {},
   "source": [
    "## TODO \n",
    "- maybe integrate the arxiv api and the similarity computation together (you can use the user query to use in the arxiv api)\n",
    "- somehow combine a chatbot with the retrieved papers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fa94b86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0dcbae39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marko\\AppData\\Local\\Temp\\ipykernel_24004\\3637962170.py:17: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  results = list(search.results())  # Convert generator to list\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank #1\n",
      "Title: Pretrained Embeddings for E-commerce Machine Learning: When it Fails and Why?\n",
      "Authors: Da Xu, Bo Yang\n",
      "Summary: The use of pretrained embeddings has become widespread in modern e-commerce\n",
      "machine learning (ML) systems. In practice, however, we have encountered\n",
      "several key issues when using pretrained embedding in a real-world production\n",
      "system, many of which cannot be fully explained by current knowledge.\n",
      "Unfortunately, we find that there is a lack of a thorough understanding of how\n",
      "pre-trained embeddings work, especially their intrinsic properties and\n",
      "interactions with downstream tasks. Consequently, it becomes challenging to\n",
      "make interactive and scalable decisions regarding the use of pre-trained\n",
      "embeddings in practice.\n",
      "  Our investigation leads to two significant discoveries about using pretrained\n",
      "embeddings in e-commerce applications. Firstly, we find that the design of the\n",
      "pretraining and downstream models, particularly how they encode and decode\n",
      "information via embedding vectors, can have a profound impact. Secondly, we\n",
      "establish a principled perspective of pre-trained embeddings via the lens of\n",
      "kernel analysis, which can be used to evaluate their predictability,\n",
      "interactively and scalably. These findings help to address the practical\n",
      "challenges we faced and offer valuable guidance for successful adoption of\n",
      "pretrained embeddings in real-world production. Our conclusions are backed by\n",
      "solid theoretical reasoning, benchmark experiments, as well as online testings.\n",
      "Similarity: 0.5412\n",
      "URL: https://arxiv.org/abs/2304.04330v1\n",
      "--------------------------------------------------------------------------------\n",
      "Rank #2\n",
      "Title: Force2Vec: Parallel force-directed graph embedding\n",
      "Authors: Md. Khaledur Rahman, Majedul Haque Sujon, Ariful Azad\n",
      "Summary: A graph embedding algorithm embeds a graph into a low-dimensional space such\n",
      "that the embedding preserves the inherent properties of the graph. While graph\n",
      "embedding is fundamentally related to graph visualization, prior work did not\n",
      "exploit this connection explicitly. We develop Force2Vec that uses\n",
      "force-directed graph layout models in a graph embedding setting with an aim to\n",
      "excel in both machine learning (ML) and visualization tasks. We make Force2Vec\n",
      "highly parallel by mapping its core computations to linear algebra and\n",
      "utilizing multiple levels of parallelism available in modern processors. The\n",
      "resultant algorithm is an order of magnitude faster than existing methods (43x\n",
      "faster than DeepWalk, on average) and can generate embeddings from graphs with\n",
      "billions of edges in a few hours. In comparison to existing methods, Force2Vec\n",
      "is better in graph visualization and performs comparably or better in ML tasks\n",
      "such as link prediction, node classification, and clustering. Source code is\n",
      "available at https://github.com/HipGraph/Force2Vec.\n",
      "Similarity: 0.4853\n",
      "URL: https://arxiv.org/abs/2009.10035v1\n",
      "--------------------------------------------------------------------------------\n",
      "Rank #3\n",
      "Title: Efficient Domain Adaptation of Multimodal Embeddings using Constrastive Learning\n",
      "Authors: Georgios Margaritis, Periklis Petridis, Dimitris J. Bertsimas\n",
      "Summary: Recent advancements in machine learning (ML), natural language processing\n",
      "(NLP), and foundational models have shown promise for real-life applications in\n",
      "critical, albeit compute-constrainted fields like healthcare.\n",
      "  In such areas, combining foundational models with supervised ML offers\n",
      "potential for automating tasks like diagnosis and treatment planning, but the\n",
      "limited availability of onsite computational resources pose significant\n",
      "challenges before applying these technologies effectively: Current approaches\n",
      "either yield subpar results when using pretrained models without task-specific\n",
      "adaptation, or require substantial computational resources for fine-tuning,\n",
      "which is often a barrier to entry in such environments.\n",
      "  This renders them inaccessible in applications where performance and quality\n",
      "standards are high, but computational resources are scarce.\n",
      "  To bridge the gap between best-in-class performance and accessibility, we\n",
      "propose a novel method for adapting foundational, multimodal embeddings to\n",
      "downstream tasks, without the need of expensive fine-tuning processes.\n",
      "  Our method leverages frozen embeddings from Large Language Models (LLMs) and\n",
      "Vision Models, and uses contrastive learning to train a small, task-specific\n",
      "nonlinear projection that can be used in the downstream task, without having to\n",
      "fine-tune the original foundational models.\n",
      "  We show that this efficient procedure leads to significant performance\n",
      "improvements across various downstream tasks, and perhaps more importantly with\n",
      "minimal computational overhead, offering a practical solution for the use of\n",
      "advanced, foundational ML models in resource-constrained settings.\n",
      "Similarity: 0.4822\n",
      "URL: https://arxiv.org/abs/2502.02048v1\n",
      "--------------------------------------------------------------------------------\n",
      "Rank #4\n",
      "Title: Embedding-based classifiers can detect prompt injection attacks\n",
      "Authors: Md. Ahsan Ayub, Subhabrata Majumdar\n",
      "Summary: Large Language Models (LLMs) are seeing significant adoption in every type of\n",
      "organization due to their exceptional generative capabilities. However, LLMs\n",
      "are found to be vulnerable to various adversarial attacks, particularly prompt\n",
      "injection attacks, which trick them into producing harmful or inappropriate\n",
      "content. Adversaries execute such attacks by crafting malicious prompts to\n",
      "deceive the LLMs. In this paper, we propose a novel approach based on\n",
      "embedding-based Machine Learning (ML) classifiers to protect LLM-based\n",
      "applications against this severe threat. We leverage three commonly used\n",
      "embedding models to generate embeddings of malicious and benign prompts and\n",
      "utilize ML classifiers to predict whether an input prompt is malicious. Out of\n",
      "several traditional ML methods, we achieve the best performance with\n",
      "classifiers built using Random Forest and XGBoost. Our classifiers outperform\n",
      "state-of-the-art prompt injection classifiers available in open-source\n",
      "implementations, which use encoder-only neural networks.\n",
      "Similarity: 0.4749\n",
      "URL: https://arxiv.org/abs/2410.22284v1\n",
      "--------------------------------------------------------------------------------\n",
      "Rank #5\n",
      "Title: Drug-Drug Interaction Prediction Based on Knowledge Graph Embeddings and Convolutional-LSTM Network\n",
      "Authors: Md. Rezaul Karim, Michael Cochez, Joao Bosco Jares, Mamtaz Uddin, Oya Beyan, Stefan Decker\n",
      "Summary: Interference between pharmacological substances can cause serious medical\n",
      "injuries. Correctly predicting so-called drug-drug interactions (DDI) does not\n",
      "only reduce these cases but can also result in a reduction of drug development\n",
      "cost. Presently, most drug-related knowledge is the result of clinical\n",
      "evaluations and post-marketing surveillance; resulting in a limited amount of\n",
      "information. Existing data-driven prediction approaches for DDIs typically rely\n",
      "on a single source of information, while using information from multiple\n",
      "sources would help improve predictions. Machine learning (ML) techniques are\n",
      "used, but the techniques are often unable to deal with skewness in the data.\n",
      "Hence, we propose a new ML approach for predicting DDIs based on multiple data\n",
      "sources. For this task, we use 12,000 drug features from DrugBank, PharmGKB,\n",
      "and KEGG drugs, which are integrated using Knowledge Graphs (KGs). To train our\n",
      "prediction model, we first embed the nodes in the graph using various embedding\n",
      "approaches. We found that the best performing combination was a ComplEx\n",
      "embedding method creating using PyTorch-BigGraph (PBG) with a\n",
      "Convolutional-LSTM network and classic machine learning-based prediction\n",
      "models. The model averaging ensemble method of three best classifiers yields up\n",
      "to 0.94, 0.92, 0.80 for AUPR, F1-score, and MCC, respectively during 5-fold\n",
      "cross-validation tests.\n",
      "Similarity: 0.4547\n",
      "URL: https://arxiv.org/abs/1908.01288v1\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Load the sentence transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2') \n",
    "\n",
    "# Define your query\n",
    "user_query = \"Best ML models for making embeddings\"\n",
    "\n",
    "# Get the embedding for the query\n",
    "query_embedding = model.encode([user_query])\n",
    "\n",
    "# Use arxiv to search for papers (limit to 100 results)\n",
    "search = arxiv.Search(\n",
    "    query=user_query,\n",
    "    max_results=100,\n",
    "    sort_by=arxiv.SortCriterion.Relevance,\n",
    "    sort_order=arxiv.SortOrder.Descending\n",
    "      )\n",
    "results = list(search.results())  # Convert generator to list\n",
    "\n",
    "# Extract summaries and titles\n",
    "papers = []\n",
    "summaries = []\n",
    "for result in results:\n",
    "    title = result.title\n",
    "    authors = ', '.join([author.name for author in result.authors])\n",
    "    summary = result.summary\n",
    "    url = f\"https://arxiv.org/abs/{result.entry_id.split('/')[-1]}\"\n",
    "    papers.append({\n",
    "        \"title\": title,\n",
    "        \"authors\": authors,\n",
    "        \"summary\": summary,\n",
    "        \"url\": url\n",
    "    })\n",
    "    summaries.append(summary)\n",
    "\n",
    "# Encode all summaries\n",
    "summary_embeddings = model.encode(summaries)\n",
    "\n",
    "# Compute cosine similarities\n",
    "similarities = cosine_similarity(query_embedding, summary_embeddings)[0]\n",
    "\n",
    "# Attach similarity scores to papers and sort\n",
    "for i, paper in enumerate(papers):\n",
    "    paper[\"similarity\"] = similarities[i]\n",
    "\n",
    "top_papers = sorted(papers, key=lambda x: x[\"similarity\"], reverse=True)[:5]\n",
    "\n",
    "# Print top 5 similar papers\n",
    "for i, paper in enumerate(top_papers, 1):\n",
    "    print(f\"Rank #{i}\")\n",
    "    print(f\"Title: {paper['title']}\")\n",
    "    print(f\"Authors: {paper['authors']}\")\n",
    "    print(f\"Summary: {paper['summary']}\")\n",
    "    print(f\"Similarity: {paper['similarity']:.4f}\")\n",
    "    print(f\"URL: {paper['url']}\")\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3c9bbaa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1486 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Research Papers and Generated Answer:\n",
      "Research Papers:\n",
      "Title: Pretrained Embeddings for E-commerce Machine Learning: When it Fails and Why?\n",
      "Summary: The use of pretrained embeddings has become widespread in modern e-commerce\n",
      "machine learning (ML) systems. In practice, however, we have encountered\n",
      "several key issues when using pretrained embedding in a real-world production\n",
      "system, many of which cannot be fully explained by current knowledge.\n",
      "Unfortunately, we find that there is a lack of a thorough understanding of how\n",
      "pre-trained embeddings work, especially th\n",
      "Generated Answer:\n",
      "Pretrained Embeddings for E-commerce Machine Learning: When it Fails and Why?\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "\n",
    "model_id = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
    "\n",
    "rag = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Combine summaries into a context string, but make sure it's within the token limit\n",
    "context = \"\\n\\n\".join(\n",
    "    f\"Title: {paper['title']}\\nSummary: {paper['summary']}\" for paper in top_papers\n",
    ")\n",
    "\n",
    "# Encode the context and check its length\n",
    "input_ids = tokenizer.encode(context, return_tensors=\"pt\")\n",
    "max_length = 512  # Adjust this based on your model's max token length\n",
    "\n",
    "# Truncate if necessary to fit within the max token limit\n",
    "if input_ids.shape[1] > max_length:\n",
    "    input_ids = input_ids[:, :max_length]\n",
    "\n",
    "\n",
    "# Prepare the prompt, ensuring it stays within the token limit\n",
    "prompt = f\"\"\"Here are some research papers:\n",
    "\n",
    "{context[:max_length]}  # Only include a truncated context if necessary\n",
    "\n",
    "Use the above research paper summaries to answer the following question:\n",
    "\n",
    "Question: {user_query}\n",
    "Answer:\"\"\"\n",
    "\n",
    "# Generate the answer using the same prompt\n",
    "output = rag(prompt, max_new_tokens=300)\n",
    "\n",
    "# Provide the generated answer along with the papers\n",
    "print(\"Research Papers and Generated Answer:\")\n",
    "print(f\"Research Papers:\\n{context[:max_length]}\")  # Display truncated context\n",
    "print(f\"Generated Answer:\\n{output[0]['generated_text']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
