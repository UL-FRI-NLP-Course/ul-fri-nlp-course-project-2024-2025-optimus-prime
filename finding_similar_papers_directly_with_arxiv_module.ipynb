{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf4c9f8f",
   "metadata": {},
   "source": [
    "## TODO \n",
    "- maybe some better model for embedding extraction\n",
    "- better prompt for the chatbot \n",
    "- somehow test the implementation\n",
    "- Adding citations to score the papers\n",
    "- somehow separate the user query and searching for papers on arxiv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddeab2f3",
   "metadata": {},
   "source": [
    "## Generate a response by incoroprating the retrieved papers with a chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c8b731",
   "metadata": {},
   "source": [
    "## Larger model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff40bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sebas\\one\\OneDrive\\grive\\faks\\masters\\y1\\2nd semester\\NLP\\ul-fri-nlp-course-project-2024-2025-optimus-prime\\.venv\\lib\\site-packages\\accelerate\\utils\\modeling.py:1569: UserWarning: Current model requires 32.0 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 26\u001b[0m\n\u001b[0;32m     16\u001b[0m bnb_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[0;32m     17\u001b[0m     load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;66;03m# loading in 4 bit\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     bnb_4bit_quant_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnf4\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m# quantization type\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     bnb_4bit_use_double_quant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;66;03m# nested quantization\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     bnb_4bit_compute_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16,\n\u001b[0;32m     21\u001b[0m )\n\u001b[0;32m     23\u001b[0m model_config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m     24\u001b[0m     pretrained_model_name_or_path\u001b[38;5;241m=\u001b[39mLLM_MODEL,\n\u001b[0;32m     25\u001b[0m )\n\u001b[1;32m---> 26\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLLM_MODEL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# we introduce the bnb config here.\u001b[39;49;00m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32mc:\\Users\\sebas\\one\\OneDrive\\grive\\faks\\masters\\y1\\2nd semester\\NLP\\ul-fri-nlp-course-project-2024-2025-optimus-prime\\.venv\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:571\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mconfig_class \u001b[38;5;241m==\u001b[39m config\u001b[38;5;241m.\u001b[39msub_configs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    570\u001b[0m         config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_text_config()\n\u001b[1;32m--> 571\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    572\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    573\u001b[0m     )\n\u001b[0;32m    574\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    575\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    576\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    577\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\sebas\\one\\OneDrive\\grive\\faks\\masters\\y1\\2nd semester\\NLP\\ul-fri-nlp-course-project-2024-2025-optimus-prime\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:279\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    281\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[1;32mc:\\Users\\sebas\\one\\OneDrive\\grive\\faks\\masters\\y1\\2nd semester\\NLP\\ul-fri-nlp-course-project-2024-2025-optimus-prime\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:4380\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   4378\u001b[0m \u001b[38;5;66;03m# Prepare the full device map\u001b[39;00m\n\u001b[0;32m   4379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4380\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m \u001b[43m_get_device_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4382\u001b[0m \u001b[38;5;66;03m# Finalize model weight initialization\u001b[39;00m\n\u001b[0;32m   4383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m from_tf:\n",
      "File \u001b[1;32mc:\\Users\\sebas\\one\\OneDrive\\grive\\faks\\masters\\y1\\2nd semester\\NLP\\ul-fri-nlp-course-project-2024-2025-optimus-prime\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:1304\u001b[0m, in \u001b[0;36m_get_device_map\u001b[1;34m(model, device_map, max_memory, hf_quantizer, torch_dtype, keep_in_fp32_regex)\u001b[0m\n\u001b[0;32m   1301\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m infer_auto_device_map(model, dtype\u001b[38;5;241m=\u001b[39mtarget_dtype, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdevice_map_kwargs)\n\u001b[0;32m   1303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1304\u001b[0m         \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1306\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1307\u001b[0m     tied_params \u001b[38;5;241m=\u001b[39m find_tied_parameters(model)\n",
      "File \u001b[1;32mc:\\Users\\sebas\\one\\OneDrive\\grive\\faks\\masters\\y1\\2nd semester\\NLP\\ul-fri-nlp-course-project-2024-2025-optimus-prime\\.venv\\lib\\site-packages\\transformers\\quantizers\\quantizer_bnb_4bit.py:104\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m--> 104\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    105\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    106\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    107\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    108\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`from_pretrained`. Check \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    109\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    110\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor more details. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    111\u001b[0m         )\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(importlib\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mversion(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbitsandbytes\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;241m<\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.39.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    115\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have a version of `bitsandbytes` that is not compatible with 4bit inference and training\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    116\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you have the latest version of `bitsandbytes` installed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    117\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig, AutoConfig\n",
    "import torch\n",
    "import dotenv\n",
    "\n",
    "# Load a chat-capable model\n",
    "LLM_MODEL = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "device = f\"cuda:{torch.cuda.current_device()}\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=LLM_MODEL,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# 8-bit quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, # loading in 4 bit\n",
    "    bnb_4bit_quant_type=\"nf4\", # quantization type\n",
    "    bnb_4bit_use_double_quant=True, # nested quantization\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model_config = AutoConfig.from_pretrained(\n",
    "    pretrained_model_name_or_path=LLM_MODEL,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=LLM_MODEL,\n",
    "    config=model_config,\n",
    "    quantization_config=bnb_config, # we introduce the bnb config here.\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97fc93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_3356\\2378073949.py:16: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=generate_text)\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Define the Hugging Face pipeline for text generation\n",
    "generate_text = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,\n",
    "    max_new_tokens=8192,\n",
    "    repetition_penalty=1.1,\n",
    ")\n",
    "\n",
    "# Wrap the Hugging Face pipeline into LangChain's LLM\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)\n",
    "\n",
    "template = \"\"\"\n",
    "You are a helpful AI QA assistant, for answering queries about research methods.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e16615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a helpful AI QA assistant, for answering querries about research methods.\n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "Question: Which paper introduced the transformer architecture\n",
      "Answer: The transformer architecture was introduced in the paper \"Attention is All You Need\" by Vaswani et al., published in 2017.\n"
     ]
    }
   ],
   "source": [
    "# an example of something that works without rag\n",
    "chain = prompt | llm\n",
    "question = \"Which paper introduced the transformer architecture\"\n",
    "print(chain.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b84fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a helpful AI QA assistant, for answering querries about research methods.\n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "Question: \n",
      "Answer: I'd be happy to help answer any questions you have about research methods! However, your question is not specific enough for me to provide a clear answer. Could you please specify which research method or methods you are inquiring about? Some common research methods include surveys, experiments, case studies, and literature reviews. Once I have more information, I can provide a more accurate response.\n"
     ]
    }
   ],
   "source": [
    "# A newer one \n",
    "question = \"\"\n",
    "print(chain.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb5a202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a helpful AI QA assistant, for answering querries about research methods.\n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "Question: What is the latest training data you have been trained on?\n",
      "Answer: I don't have the ability to be trained on data or to have a specific training dataset. I am a text-based AI model and do not rely on data to generate responses. I am designed to process and understand natural language input and provide relevant information based on that input and my programming.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the latest training data you have been trained on?\"\n",
    "print(chain.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccfea2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a helpful AI QA assistant, for answering querries about research methods.\n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "Question: What can you tell me about the paper Instruct-ReID: A Multi-purpose Person Re-identification Task with Instructions\n",
      "Answer: \"Instruct-ReID\" is a paper published in the IEEE Transactions on Pattern Analysis and Machine Intelligence journal. The authors propose a new multi-purpose person re-identification (ReID) task, which involves not only identifying the same person across different cameras but also understanding and following given instructions related to the person. This task is designed to evaluate the ability of models to reason about context and follow instructions, in addition to their ability to recognize people. The paper provides a detailed description of the dataset and evaluation protocol used for this task. If you need more specific information, please let me know.\n"
     ]
    }
   ],
   "source": [
    "question = \"What can you tell me about the paper Instruct-ReID: A Multi-purpose Person Re-identification Task with Instructions\"\n",
    "print(chain.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67284e4",
   "metadata": {},
   "source": [
    "## Get the papers based on the user query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcbae39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sebas\\one\\OneDrive\\grive\\faks\\masters\\y1\\2nd semester\\NLP\\ul-fri-nlp-course-project-2024-2025-optimus-prime\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\sebas\\.cache\\huggingface\\hub\\models--sentence-transformers--allenai-specter. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank #1\n",
      "Title: Position: Foundation Agents as the Paradigm Shift for Decision Making\n",
      "Authors: Xiaoqian Liu, Xingzhou Lou, Jianbin Jiao, Junge Zhang\n",
      "Summary: Decision making demands intricate interplay between perception, memory, and\n",
      "reasoning to discern optimal policies. Conventional approaches to decision\n",
      "making face challenges related to low sample efficiency and poor\n",
      "generalization. In contrast, foundation models in language and vision have\n",
      "showcased rapid adaptation to diverse new tasks. Therefore, we advocate for the\n",
      "construction of foundation agents as a transformative shift in the learning\n",
      "paradigm of agents. This proposal is underpinned by the formulation of\n",
      "foundation agents with their fundamental characteristics and challenges\n",
      "motivated by the success of large language models (LLMs). Moreover, we specify\n",
      "the roadmap of foundation agents from large interactive data collection or\n",
      "generation, to self-supervised pretraining and adaptation, and knowledge and\n",
      "value alignment with LLMs. Lastly, we pinpoint critical research questions\n",
      "derived from the formulation and delineate trends for foundation agents\n",
      "supported by real-world use cases, addressing both technical and theoretical\n",
      "aspects to propel the field towards a more comprehensive and impactful future.\n",
      "Similarity: 0.8611\n",
      "URL: https://arxiv.org/abs/2405.17009v3\n",
      "--------------------------------------------------------------------------------\n",
      "Rank #2\n",
      "Title: Toward Efficient Exploration by Large Language Model Agents\n",
      "Authors: Dilip Arumugam, Thomas L. Griffiths\n",
      "Summary: A burgeoning area within reinforcement learning (RL) is the design of\n",
      "sequential decision-making agents centered around large language models (LLMs).\n",
      "While autonomous decision-making agents powered by modern LLMs could facilitate\n",
      "numerous real-world applications, such successes demand agents that are capable\n",
      "of data-efficient RL. One key obstacle to achieving data efficiency in RL is\n",
      "exploration, a challenge that we demonstrate many recent proposals for LLM\n",
      "agent designs struggle to contend with. Meanwhile, classic algorithms from the\n",
      "RL literature known to gracefully address exploration require technical\n",
      "machinery that can be challenging to operationalize in purely natural language\n",
      "settings. In this work, rather than relying on finetuning or in-context\n",
      "learning to coax LLMs into implicitly imitating a RL algorithm, we illustrate\n",
      "how LLMs can be used to explicitly implement an existing RL algorithm\n",
      "(Posterior Sampling for Reinforcement Learning) whose capacity for\n",
      "statistically-efficient exploration is already well-studied. We offer empirical\n",
      "results demonstrating how our LLM-based implementation of a known,\n",
      "data-efficient RL algorithm can be considerably more effective in natural\n",
      "language tasks that demand prudent exploration.\n",
      "Similarity: 0.8584\n",
      "URL: https://arxiv.org/abs/2504.20997v1\n",
      "--------------------------------------------------------------------------------\n",
      "Rank #3\n",
      "Title: AgentGym: Evolving Large Language Model-based Agents across Diverse Environments\n",
      "Authors: Zhiheng Xi, Yiwen Ding, Wenxiang Chen, Boyang Hong, Honglin Guo, Junzhe Wang, Dingwen Yang, Chenyang Liao, Xin Guo, Wei He, Songyang Gao, Lu Chen, Rui Zheng, Yicheng Zou, Tao Gui, Qi Zhang, Xipeng Qiu, Xuanjing Huang, Zuxuan Wu, Yu-Gang Jiang\n",
      "Summary: Building generalist agents that can handle diverse tasks and evolve\n",
      "themselves across different environments is a long-term goal in the AI\n",
      "community. Large language models (LLMs) are considered a promising foundation\n",
      "to build such agents due to their generalized capabilities. Current approaches\n",
      "either have LLM-based agents imitate expert-provided trajectories step-by-step,\n",
      "requiring human supervision, which is hard to scale and limits environmental\n",
      "exploration; or they let agents explore and learn in isolated environments,\n",
      "resulting in specialist agents with limited generalization. In this paper, we\n",
      "take the first step towards building generally-capable LLM-based agents with\n",
      "self-evolution ability. We identify a trinity of ingredients: 1) diverse\n",
      "environments for agent exploration and learning, 2) a trajectory set to equip\n",
      "agents with basic capabilities and prior knowledge, and 3) an effective and\n",
      "scalable evolution method. We propose AgentGym, a new framework featuring a\n",
      "variety of environments and tasks for broad, real-time, uni-format, and\n",
      "concurrent agent exploration. AgentGym also includes a database with expanded\n",
      "instructions, a benchmark suite, and high-quality trajectories across\n",
      "environments. Next, we propose a novel method, AgentEvol, to investigate the\n",
      "potential of agent self-evolution beyond previously seen data across tasks and\n",
      "environments. Experimental results show that the evolved agents can achieve\n",
      "results comparable to SOTA models. We release the AgentGym suite, including the\n",
      "platform, dataset, benchmark, checkpoints, and algorithm implementations. The\n",
      "AgentGym suite is available on https://github.com/WooooDyy/AgentGym.\n",
      "Similarity: 0.8563\n",
      "URL: https://arxiv.org/abs/2406.04151v1\n",
      "--------------------------------------------------------------------------------\n",
      "Rank #4\n",
      "Title: Controlling Large Language Model Agents with Entropic Activation Steering\n",
      "Authors: Nate Rahn, Pierluca D'Oro, Marc G. Bellemare\n",
      "Summary: The rise of large language models (LLMs) has prompted increasing interest in\n",
      "their use as in-context learning agents. At the core of agentic behavior is the\n",
      "capacity for exploration, or the ability to actively gather information about\n",
      "the environment. But how do LLM agents explore, and how can we control their\n",
      "exploratory behaviors? To answer these questions, we take a\n",
      "representation-level perspective, and introduce Entropic Activation Steering\n",
      "(EAST), an activation steering method for in-context LLM agents. Firstly, we\n",
      "demonstrate that EAST can effectively manipulate an LLM agent's exploration by\n",
      "directly affecting the high-level actions parsed from the outputs of the LLM,\n",
      "in contrast to token-level temperature sampling. Secondly, we reveal how\n",
      "applying this control modulates the uncertainty exhibited in the LLM's\n",
      "thoughts, guiding the agent towards more exploratory actions. Finally, we\n",
      "demonstrate that the steering vectors obtained by EAST generalize across task\n",
      "variants. In total, these results show that LLM agents explicitly encode\n",
      "uncertainty over their actions in their representation space. Our work paves\n",
      "the way for a new understanding of the functioning of LLM agents and to\n",
      "effective control of their decision-making behaviors.\n",
      "Similarity: 0.8555\n",
      "URL: https://arxiv.org/abs/2406.00244v2\n",
      "--------------------------------------------------------------------------------\n",
      "Rank #5\n",
      "Title: Mutual Enhancement of Large Language and Reinforcement Learning Models through Bi-Directional Feedback Mechanisms: A Planning Case Study\n",
      "Authors: Shangding Gu\n",
      "Summary: Large Language Models (LLMs) have demonstrated remarkable capabilities for\n",
      "reinforcement learning (RL) models, such as planning and reasoning\n",
      "capabilities. However, the problems of LLMs and RL model collaboration still\n",
      "need to be solved. In this study, we employ a teacher-student learning\n",
      "framework to tackle these problems, specifically by offering feedback for LLMs\n",
      "using RL models and providing high-level information for RL models with LLMs in\n",
      "a cooperative multi-agent setting. Within this framework, the LLM acts as a\n",
      "teacher, while the RL model acts as a student. The two agents cooperatively\n",
      "assist each other through a process of recursive help, such as \"I help you help\n",
      "I help.\" The LLM agent supplies abstract information to the RL agent, enabling\n",
      "efficient exploration and policy improvement. In turn, the RL agent offers\n",
      "feedback to the LLM agent, providing valuable, real-time information that helps\n",
      "generate more useful tokens. This bi-directional feedback loop promotes\n",
      "optimization, exploration, and mutual improvement for both agents, enabling\n",
      "them to accomplish increasingly challenging tasks. Remarkably, we propose a\n",
      "practical algorithm to address the problem and conduct empirical experiments to\n",
      "evaluate the effectiveness of our method.\n",
      "Similarity: 0.8529\n",
      "URL: https://arxiv.org/abs/2401.06603v2\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer \n",
    "import arxiv \n",
    "\n",
    "# Load the sentence transformer model\n",
    "# TODO maybe some better model for embedding extraction, maybe we could fine tune?\n",
    "# TODO maybe somehow add citations\n",
    "# model_embeddings = SentenceTransformer('all-mpnet-base-v2')  # For embeddin extraction\n",
    "model_embeddings = SentenceTransformer('allenai-specter') # It can be used to map the titles & abstracts of scientific publications to a vector space such that similar papers are close.\n",
    "\n",
    "# Define your query\n",
    "user_query = \"Toward Efficient Exploration by Large Language Model Agents\"  # arxiv actually finds this one\n",
    "\n",
    "# Get the embedding for the query\n",
    "query_embedding = model_embeddings.encode([user_query])\n",
    "\n",
    "search = arxiv.Search(\n",
    "    query=user_query,\n",
    "    max_results=50,\n",
    "    sort_by=arxiv.SortCriterion.Relevance,\n",
    "    sort_order=arxiv.SortOrder.Descending\n",
    ")\n",
    "\n",
    "client = arxiv.Client()\n",
    "results = list(client.results(search))\n",
    "\n",
    "# Extract summaries and titles\n",
    "papers = []\n",
    "summaries = []\n",
    "for result in results:\n",
    "    title = result.title\n",
    "    authors = ', '.join([author.name for author in result.authors])\n",
    "    summary = result.summary\n",
    "    url = f\"https://arxiv.org/abs/{result.entry_id.split('/')[-1]}\"\n",
    "    papers.append({\n",
    "        \"title\": title,\n",
    "        \"authors\": authors,\n",
    "        \"summary\": summary,\n",
    "        \"url\": url\n",
    "    })\n",
    "    summaries.append(summary)\n",
    "\n",
    "# Encode all summaries\n",
    "summary_embeddings = model_embeddings.encode(summaries)\n",
    "\n",
    "# Compute cosine similarities\n",
    "similarities = cosine_similarity(query_embedding, summary_embeddings)[0]\n",
    "\n",
    "for i, paper in enumerate(papers):\n",
    "    paper[\"similarity\"] = similarities[i]\n",
    "\n",
    "\n",
    "top_papers = sorted(papers, key=lambda x: x[\"similarity\"], reverse=True)[:5] # top 5\n",
    "\n",
    "# Print top 5 similar papers\n",
    "for i, paper in enumerate(top_papers, 1):\n",
    "    print(f\"Rank #{i}\")\n",
    "    print(f\"Title: {paper['title']}\")\n",
    "    print(f\"Authors: {paper['authors']}\")\n",
    "    print(f\"Summary: {paper['summary']}\")\n",
    "    print(f\"Similarity: {paper['similarity']:.4f}\")\n",
    "    print(f\"URL: {paper['url']}\")\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979439cc",
   "metadata": {},
   "source": [
    "## Combine the retrieved papers and the generation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc93f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The model 'T5ForConditionalGeneration' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DeepseekV3ForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'Llama4ForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'Phi4MultimodalForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: You are a helpful AI QA assistant, for answering querries about research methods.\n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "```\n",
      "Title: Instruct-ReID++: Towards Universal Purpose Instruction-Guided Person Re-identification\n",
      "Summary: Human intelligence can retrieve any person according to both visual and\n",
      "language descriptions. However, the current computer vision community studies\n",
      "specific person re-identification (ReID) tasks in different scenarios\n",
      "separately, which limits the applications in the real world. This paper strives\n",
      "to resolve this problem by proposing a novel instruct-ReID task that requires\n",
      "the model to retrieve images according to the given image or language\n",
      "instructions. Instruct-ReID is the first exploration of a general ReID setting,\n",
      "where existing 6 ReID tasks can be viewed as special cases by assigning\n",
      "different instructions. To facilitate research in this new instruct-ReID task,\n",
      "we propose a large-scale OmniReID++ benchmark equipped with diverse data and\n",
      "comprehensive evaluation methods e.g., task specific and task-free evaluation\n",
      "settings. In the task-specific evaluation setting, gallery sets are categorized\n",
      "according to specific ReID tasks. We propose a novel baseline model, IRM, with\n",
      "an adaptive triplet loss to handle various retrieval tasks within a unified\n",
      "framework. For task-free evaluation setting, where target person images are\n",
      "retrieved from task-agnostic gallery sets, we further propose a new method\n",
      "called IRM++ with novel memory bank-assisted learning. Extensive evaluations of\n",
      "IRM and IRM++ on OmniReID++ benchmark demonstrate the superiority of our\n",
      "proposed methods, achieving state-of-the-art performance on 10 test sets. The\n",
      "datasets, the model, and the code will be available at\n",
      "https://github.com/hwz-zju/Instruct-ReID\n",
      "\n",
      "Title: Person Re-identification: Past, Present and Future\n",
      "Summary: Person re-identification (re-ID) has become increasingly popular in the\n",
      "community due to its application and research significance. It aims at spotting\n",
      "a person of interest in other cameras. In the early days, hand-crafted\n",
      "algorithms and small-scale evaluation were predominantly reported. Recent years\n",
      "have witnessed the emergence of large-scale datasets and deep learning systems\n",
      "which make use of large data volumes. Considering different tasks, we classify\n",
      "most current re-ID methods into two classes, i.e., image-based and video-based;\n",
      "in both tasks, hand-crafted and deep learning systems will be reviewed.\n",
      "Moreover, two new re-ID tasks which are much closer to real-world applications\n",
      "are described and discussed, i.e., end-to-end re-ID and fast re-ID in very\n",
      "large galleries. This paper: 1) introduces the history of person re-ID and its\n",
      "relationship with image classification and instance retrieval; 2) surveys a\n",
      "broad selection of the hand-crafted systems and the large-scale methods in both\n",
      "image- and video-based re-ID; 3) describes critical future directions in\n",
      "end-to-end re-ID and fast retrieval in large galleries; and 4) finally briefs\n",
      "some important yet under-developed issues.\n",
      "\n",
      "Title: TeLL Me what you cant see\n",
      "Summary: During criminal investigations, images of persons of interest directly\n",
      "influence the success of identification procedures. However, law enforcement\n",
      "agencies often face challenges related to the scarcity of high-quality images\n",
      "or their obsolescence, which can affect the accuracy and success of people\n",
      "searching processes. This paper introduces a novel forensic mugshot\n",
      "augmentation framework aimed at addressing these limitations. Our approach\n",
      "enhances the identification probability of individuals by generating\n",
      "additional, high-quality images through customizable data augmentation\n",
      "techniques, while maintaining the biometric integrity and consistency of the\n",
      "original data. Several experimental results show that our method significantly\n",
      "improves identification accuracy and robustness across various forensic\n",
      "scenarios, demonstrating its effectiveness as a trustworthy tool law\n",
      "enforcement applications. Index Terms: Digital Forensics, Person\n",
      "re-identification, Feature extraction, Data augmentation, Visual-Language\n",
      "models.\n",
      "\n",
      "Title: Unsupervised Domain Generalization for Person Re-identification: A Domain-specific Adaptive Framework\n",
      "Summary: Domain generalization (DG) has attracted much attention in person\n",
      "re-identification (ReID) recently. It aims to make a model trained on multiple\n",
      "source domains generalize to an unseen target domain. Although achieving\n",
      "promising progress, existing methods usually need the source domains to be\n",
      "labeled, which could be a significant burden for practical ReID tasks. In this\n",
      "paper, we turn to investigate unsupervised domain generalization for ReID, by\n",
      "assuming that no label is available for any source domains.\n",
      "  To address this challenging setting, we propose a simple and efficient\n",
      "domain-specific adaptive framework, and realize it with an adaptive\n",
      "normalization module designed upon the batch and instance normalization\n",
      "techniques. In doing so, we successfully yield reliable pseudo-labels to\n",
      "implement training and also enhance the domain generalization capability of the\n",
      "model as required. In addition, we show that our framework can even be applied\n",
      "to improve person ReID under the settings of supervised domain generalization\n",
      "and unsupervised domain adaptation, demonstrating competitive performance with\n",
      "respect to relevant methods. Extensive experimental study on benchmark datasets\n",
      "is conducted to validate the proposed framework. A significance of our work\n",
      "lies in that it shows the potential of unsupervised domain generalization for\n",
      "person ReID and sets a strong baseline for the further research on this topic.\n",
      "\n",
      "Title: Deep Spatial Feature Reconstruction for Partial Person Re-identification: Alignment-Free Approach\n",
      "Summary: Partial person re-identification (re-id) is a challenging problem, where only\n",
      "several partial observations (images) of people are available for matching.\n",
      "However, few studies have provided flexible solutions to identifying a person\n",
      "in an image containing arbitrary part of the body. In this paper, we propose a\n",
      "fast and accurate matching method to address this problem. The proposed method\n",
      "leverages Fully Convolutional Network (FCN) to generate fix-sized spatial\n",
      "feature maps such that pixel-level features are consistent. To match a pair of\n",
      "person images of different sizes, a novel method called Deep Spatial feature\n",
      "Reconstruction (DSR) is further developed to avoid explicit alignment.\n",
      "Specifically, DSR exploits the reconstructing error from popular dictionary\n",
      "learning models to calculate the similarity between different spatial feature\n",
      "maps. In that way, we expect that the proposed FCN can decrease the similarity\n",
      "of coupled images from different persons and increase that from the same\n",
      "person. Experimental results on two partial person datasets demonstrate the\n",
      "efficiency and effectiveness of the proposed method in comparison with several\n",
      "state-of-the-art partial person re-id approaches. Additionally, DSR achieves\n",
      "competitive results on a benchmark person dataset Market1501 with 83.58\\%\n",
      "Rank-1 accuracy.\n",
      "```\n",
      "\n",
      "### Question:\n",
      "What can you tell me about the paper Instruct-ReID: A Multi-purpose Person Re-identification Task with Instructions\n",
      "\n",
      "### Answer:\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Combine summaries into a context string\n",
    "context = \"\\n\\n\".join(\n",
    "    f\"Title: {paper['title']}\\nSummary: {paper['summary']}\" for paper in top_papers\n",
    ")\n",
    "\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "You are a helpful AI QA assistant, for answering querries about research methods.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "```\n",
    "{context}\n",
    "```\n",
    "\n",
    "### Question:\n",
    "{question}\n",
    "\n",
    "### Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=PROMPT_TEMPLATE.strip(),\n",
    ")\n",
    "\n",
    "# Define the Hugging Face pipeline for text generation\n",
    "generate_text = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=model,  # Replace with your model\n",
    "    tokenizer=tokenizer,  # Replace with your tokenizer\n",
    "    return_full_text=True,\n",
    "    max_new_tokens=8192,\n",
    "    repetition_penalty=1.1,\n",
    ")\n",
    "\n",
    "# Wrap the Hugging Face pipeline into LangChain's LLM\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)\n",
    "\n",
    "# Create the LLMChain with the prompt template and the LLM\n",
    "qa_chain = LLMChain(prompt=prompt_template, llm=llm)\n",
    "\n",
    "# Ask the model a question and get the answer\n",
    "question = user_query # TODO maybe change this, to make it different than the search (or change the search)\n",
    "response = qa_chain.run({\"context\": context, \"question\": question})\n",
    "\n",
    "# Print the response\n",
    "print(\"Answer:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54446b81",
   "metadata": {},
   "source": [
    "## Simpler model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bba74e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sebas\\one\\OneDrive\\grive\\faks\\masters\\y1\\2nd semester\\NLP\\ul-fri-nlp-course-project-2024-2025-optimus-prime\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\sebas\\.cache\\huggingface\\hub\\models--google--flan-t5-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Device set to use cuda:0\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1065 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Research Papers and Generated Answer:\n",
      "Research Papers:\n",
      "Title: Neuromodulation Gated Transformer\n",
      "Summary: We introduce a novel architecture, the Neuromodulation Gated Transformer\n",
      "(NGT), which is a simple implementation of neuromodulation in transformers via\n",
      "a multiplicative effect. We compare it to baselines and show that it results in\n",
      "the best average performance on the SuperGLUE benchmark validation sets.\n",
      "\n",
      "Title: Interpretation of the Transformer and Improvement of the Extractor\n",
      "Summary: It has been over six years since the Transformer architecture was put\n",
      "forward. Surprisingly, the vanilla Transformer architecture is still widely\n",
      "used today. One reason is that the lack of deep understanding and comprehensive\n",
      "interpretation of the Transformer architecture makes it more challenging to\n",
      "improve the Transformer architecture. In this paper, we first interpret the\n",
      "Transformer architecture comprehensively in plain words based on our\n",
      "understanding and experiences. The interpretations are further proved and\n",
      "verified. These interpretations also cover the Extractor, a family of drop-in\n",
      "replacements for the multi-head self-attention in the Transformer architecture.\n",
      "Then, we propose an improvement on a type of the Extractor that outperforms the\n",
      "self-attention, without introducing additional trainable parameters.\n",
      "Experimental results demonstrate that the improved Extractor performs even\n",
      "better, showing a way to improve the Transformer architecture.\n",
      "\n",
      "Title: Atleus: Accelerating Transformers on the Edge Enabled by 3D Heterogeneous Manycore Architectures\n",
      "Summary: Transformer architectures have become the standard neural network model for\n",
      "various machine learning applications including natural language processing and\n",
      "computer vision. However,\n",
      "Generated Answer:\n",
      "Title: Atleus: Accelerating Transformers on the Edge Enabled by 3D Heterogeneous Manycore Architectures\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "\n",
    "model_id = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
    "\n",
    "rag = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Combine summaries into a context string, but make sure it's within the token limit\n",
    "context = \"\\n\\n\".join(\n",
    "    f\"Title: {paper['title']}\\nSummary: {paper['summary']}\" for paper in top_papers\n",
    ")\n",
    "\n",
    "# Encode the context and check its length\n",
    "input_ids = tokenizer.encode(context, return_tensors=\"pt\")\n",
    "max_length = 1700  # Adjust this based on your model's max token length\n",
    "\n",
    "# Truncate if necessary to fit within the max token limit\n",
    "if input_ids.shape[1] > max_length:\n",
    "    input_ids = input_ids[:, :max_length]\n",
    "\n",
    "\n",
    "# Prepare the prompt, ensuring it stays within the token limit\n",
    "prompt = f\"\"\"Here are some research papers:\n",
    "\n",
    "{context[:max_length]}  # Only include a truncated context if necessary\n",
    "\n",
    "Use the above research paper summaries to answer the following question:\n",
    "\n",
    "Question: {user_query}\n",
    "Answer:\"\"\"\n",
    "\n",
    "# Generate the answer using the same prompt\n",
    "output = rag(prompt, max_new_tokens=300)\n",
    "\n",
    "# Provide the generated answer along with the papers\n",
    "print(\"Research Papers and Generated Answer:\")\n",
    "print(f\"Research Papers:\\n{context[:max_length]}\")  # Display truncated context\n",
    "print(f\"Generated Answer:\\n{output[0]['generated_text']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLDS_HW1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
