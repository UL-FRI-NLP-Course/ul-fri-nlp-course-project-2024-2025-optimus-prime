{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf4c9f8f",
   "metadata": {},
   "source": [
    "## TODO \n",
    "- maybe some better model for embedding extraction\n",
    "- better prompt for the chatbot \n",
    "- somehow test the implementation\n",
    "- Adding citations to score the papers\n",
    "- somehow separate the user query and searching for papers on arxiv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddeab2f3",
   "metadata": {},
   "source": [
    "## Generate a response by incoroprating the retrieved papers with a chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c8b731",
   "metadata": {},
   "source": [
    "## Larger model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ff40bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marko\\miniconda3\\envs\\nlp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:27<00:00,  9.33s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): MistralRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig, AutoConfig\n",
    "import torch\n",
    "\n",
    "# Load a chat-capable model\n",
    "LLM_MODEL = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "device = f\"cuda:{torch.cuda.current_device()}\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=LLM_MODEL,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# 8-bit quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, # loading in 4 bit\n",
    "    bnb_4bit_quant_type=\"nf4\", # quantization type\n",
    "    bnb_4bit_use_double_quant=True, # nested quantization\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model_config = AutoConfig.from_pretrained(\n",
    "    pretrained_model_name_or_path=LLM_MODEL,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=LLM_MODEL,\n",
    "    config=model_config,\n",
    "    quantization_config=bnb_config, # we introduce the bnb config here.\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c97fc93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "C:\\Users\\marko\\AppData\\Local\\Temp\\ipykernel_21588\\2378073949.py:16: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=generate_text)\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Define the Hugging Face pipeline for text generation\n",
    "generate_text = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,\n",
    "    max_new_tokens=8192,\n",
    "    repetition_penalty=1.1,\n",
    ")\n",
    "\n",
    "# Wrap the Hugging Face pipeline into LangChain's LLM\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)\n",
    "\n",
    "template = \"\"\"\n",
    "You are a helpful AI QA assistant, for answering querries about research methods.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83e16615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a helpful AI QA assistant, for answering querries about research methods.\n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "Question: Which paper introduced the transformer architecture\n",
      "Answer: The transformer architecture was introduced in the paper \"Attention is All You Need\" by Vaswani et al., published in 2017.\n"
     ]
    }
   ],
   "source": [
    "# an example of something that works without rag\n",
    "chain = prompt | llm\n",
    "question = \"Which paper introduced the transformer architecture\"\n",
    "print(chain.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9b84fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a helpful AI QA assistant, for answering querries about research methods.\n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "Question: \n",
      "Answer: I'd be happy to help answer any questions you have about research methods! However, your question is not specific enough for me to provide a clear answer. Could you please specify which research method or methods you are inquiring about? Some common research methods include surveys, experiments, case studies, and literature reviews. Once I have more information, I can provide a more accurate response.\n"
     ]
    }
   ],
   "source": [
    "# A newer one \n",
    "question = \"\"\n",
    "print(chain.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67284e4",
   "metadata": {},
   "source": [
    "## Get the papers based on the user query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0dcbae39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank #1\n",
      "Title: Neuromodulation Gated Transformer\n",
      "Authors: Kobe Knowles, Joshua Bensemann, Diana Benavides-Prado, Vithya Yogarajan, Michael Witbrock, Gillian Dobbie, Yang Chen\n",
      "Summary: We introduce a novel architecture, the Neuromodulation Gated Transformer\n",
      "(NGT), which is a simple implementation of neuromodulation in transformers via\n",
      "a multiplicative effect. We compare it to baselines and show that it results in\n",
      "the best average performance on the SuperGLUE benchmark validation sets.\n",
      "Similarity: 0.5207\n",
      "URL: https://arxiv.org/abs/2305.03232v2\n",
      "--------------------------------------------------------------------------------\n",
      "Rank #2\n",
      "Title: Interpretation of the Transformer and Improvement of the Extractor\n",
      "Authors: Zhe Chen\n",
      "Summary: It has been over six years since the Transformer architecture was put\n",
      "forward. Surprisingly, the vanilla Transformer architecture is still widely\n",
      "used today. One reason is that the lack of deep understanding and comprehensive\n",
      "interpretation of the Transformer architecture makes it more challenging to\n",
      "improve the Transformer architecture. In this paper, we first interpret the\n",
      "Transformer architecture comprehensively in plain words based on our\n",
      "understanding and experiences. The interpretations are further proved and\n",
      "verified. These interpretations also cover the Extractor, a family of drop-in\n",
      "replacements for the multi-head self-attention in the Transformer architecture.\n",
      "Then, we propose an improvement on a type of the Extractor that outperforms the\n",
      "self-attention, without introducing additional trainable parameters.\n",
      "Experimental results demonstrate that the improved Extractor performs even\n",
      "better, showing a way to improve the Transformer architecture.\n",
      "Similarity: 0.4934\n",
      "URL: https://arxiv.org/abs/2311.12678v1\n",
      "--------------------------------------------------------------------------------\n",
      "Rank #3\n",
      "Title: Atleus: Accelerating Transformers on the Edge Enabled by 3D Heterogeneous Manycore Architectures\n",
      "Authors: Pratyush Dhingra, Janardhan Rao Doppa, Partha Pratim Pande\n",
      "Summary: Transformer architectures have become the standard neural network model for\n",
      "various machine learning applications including natural language processing and\n",
      "computer vision. However, the compute and memory requirements introduced by\n",
      "transformer models make them challenging to adopt for edge applications.\n",
      "Furthermore, fine-tuning pre-trained transformers (e.g., foundation models) is\n",
      "a common task to enhance the model's predictive performance on specific\n",
      "tasks/applications. Existing transformer accelerators are oblivious to\n",
      "complexities introduced by fine-tuning. In this paper, we propose the design of\n",
      "a three-dimensional (3D) heterogeneous architecture referred to as Atleus that\n",
      "incorporates heterogeneous computing resources specifically optimized to\n",
      "accelerate transformer models for the dual purposes of fine-tuning and\n",
      "inference. Specifically, Atleus utilizes non-volatile memory and systolic array\n",
      "for accelerating transformer computational kernels using an integrated 3D\n",
      "platform. Moreover, we design a suitable NoC to achieve high performance and\n",
      "energy efficiency. Finally, Atleus adopts an effective quantization scheme to\n",
      "support model compression. Experimental results demonstrate that Atleus\n",
      "outperforms existing state-of-the-art by up to 56x and 64.5x in terms of\n",
      "performance and energy efficiency respectively\n",
      "Similarity: 0.4115\n",
      "URL: https://arxiv.org/abs/2501.09588v1\n",
      "--------------------------------------------------------------------------------\n",
      "Rank #4\n",
      "Title: Spherical Position Encoding for Transformers\n",
      "Authors: Eren Unlu\n",
      "Summary: Position encoding is the primary mechanism which induces notion of sequential\n",
      "order for input tokens in transformer architectures. Even though this\n",
      "formulation in the original transformer paper has yielded plausible performance\n",
      "for general purpose language understanding and generation, several new\n",
      "frameworks such as Rotary Position Embedding (RoPE) are proposed for further\n",
      "enhancement. In this paper, we introduce the notion of \"geotokens\" which are\n",
      "input elements for transformer architectures, each representing an information\n",
      "related to a geological location. Unlike the natural language the sequential\n",
      "position is not important for the model but the geographical coordinates are.\n",
      "In order to induce the concept of relative position for such a setting and\n",
      "maintain the proportion between the physical distance and distance on embedding\n",
      "space, we formulate a position encoding mechanism based on RoPE architecture\n",
      "which is adjusted for spherical coordinates.\n",
      "Similarity: 0.4061\n",
      "URL: https://arxiv.org/abs/2310.04454v1\n",
      "--------------------------------------------------------------------------------\n",
      "Rank #5\n",
      "Title: Equivariant Neural Functional Networks for Transformers\n",
      "Authors: Viet-Hoang Tran, Thieu N. Vo, An Nguyen The, Tho Tran Huu, Minh-Khoi Nguyen-Nhat, Thanh Tran, Duy-Tung Pham, Tan Minh Nguyen\n",
      "Summary: This paper systematically explores neural functional networks (NFN) for\n",
      "transformer architectures. NFN are specialized neural networks that treat the\n",
      "weights, gradients, or sparsity patterns of a deep neural network (DNN) as\n",
      "input data and have proven valuable for tasks such as learnable optimizers,\n",
      "implicit data representations, and weight editing. While NFN have been\n",
      "extensively developed for MLP and CNN, no prior work has addressed their design\n",
      "for transformers, despite the importance of transformers in modern deep\n",
      "learning. This paper aims to address this gap by providing a systematic study\n",
      "of NFN for transformers. We first determine the maximal symmetric group of the\n",
      "weights in a multi-head attention module as well as a necessary and sufficient\n",
      "condition under which two sets of hyperparameters of the multi-head attention\n",
      "module define the same function. We then define the weight space of transformer\n",
      "architectures and its associated group action, which leads to the design\n",
      "principles for NFN in transformers. Based on these, we introduce\n",
      "Transformer-NFN, an NFN that is equivariant under this group action.\n",
      "Additionally, we release a dataset of more than 125,000 Transformers model\n",
      "checkpoints trained on two datasets with two different tasks, providing a\n",
      "benchmark for evaluating Transformer-NFN and encouraging further research on\n",
      "transformer training and performance.\n",
      "Similarity: 0.3729\n",
      "URL: https://arxiv.org/abs/2410.04209v2\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer \n",
    "import arxiv \n",
    "\n",
    "# Load the sentence transformer model\n",
    "# TODO maybe some better model for embedding extraction, maybe we could fine tune?\n",
    "# TODO maybe somehow add citations\n",
    "model_embeddings = SentenceTransformer('all-MiniLM-L6-v2')  # For embeddin extraction\n",
    "\n",
    "# Define your query\n",
    "user_query = \"Which paper introduced the transformer architecture\"\n",
    "\n",
    "# Get the embedding for the query\n",
    "query_embedding = model_embeddings.encode([user_query])\n",
    "\n",
    "search = arxiv.Search(\n",
    "    query=user_query,\n",
    "    max_results=50,\n",
    "    sort_by=arxiv.SortCriterion.Relevance,\n",
    "    sort_order=arxiv.SortOrder.Descending\n",
    ")\n",
    "\n",
    "client = arxiv.Client()\n",
    "results = list(client.results(search))\n",
    "\n",
    "# Extract summaries and titles\n",
    "papers = []\n",
    "summaries = []\n",
    "for result in results:\n",
    "    title = result.title\n",
    "    authors = ', '.join([author.name for author in result.authors])\n",
    "    summary = result.summary\n",
    "    url = f\"https://arxiv.org/abs/{result.entry_id.split('/')[-1]}\"\n",
    "    papers.append({\n",
    "        \"title\": title,\n",
    "        \"authors\": authors,\n",
    "        \"summary\": summary,\n",
    "        \"url\": url\n",
    "    })\n",
    "    summaries.append(summary)\n",
    "\n",
    "# Encode all summaries\n",
    "summary_embeddings = model_embeddings.encode(summaries)\n",
    "\n",
    "# Compute cosine similarities\n",
    "similarities = cosine_similarity(query_embedding, summary_embeddings)[0]\n",
    "\n",
    "for i, paper in enumerate(papers):\n",
    "    paper[\"similarity\"] = similarities[i]\n",
    "\n",
    "\n",
    "top_papers = sorted(papers, key=lambda x: x[\"similarity\"], reverse=True)[:5] # top 5\n",
    "\n",
    "# Print top 5 similar papers\n",
    "for i, paper in enumerate(top_papers, 1):\n",
    "    print(f\"Rank #{i}\")\n",
    "    print(f\"Title: {paper['title']}\")\n",
    "    print(f\"Authors: {paper['authors']}\")\n",
    "    print(f\"Summary: {paper['summary']}\")\n",
    "    print(f\"Similarity: {paper['similarity']:.4f}\")\n",
    "    print(f\"URL: {paper['url']}\")\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979439cc",
   "metadata": {},
   "source": [
    "## Combine the retrieved papers and the generation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc93f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: You are a helpful AI QA assistant, for answering querries about research methods.\n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "```\n",
      "Title: Neuromodulation Gated Transformer\n",
      "Summary: We introduce a novel architecture, the Neuromodulation Gated Transformer\n",
      "(NGT), which is a simple implementation of neuromodulation in transformers via\n",
      "a multiplicative effect. We compare it to baselines and show that it results in\n",
      "the best average performance on the SuperGLUE benchmark validation sets.\n",
      "\n",
      "Title: Interpretation of the Transformer and Improvement of the Extractor\n",
      "Summary: It has been over six years since the Transformer architecture was put\n",
      "forward. Surprisingly, the vanilla Transformer architecture is still widely\n",
      "used today. One reason is that the lack of deep understanding and comprehensive\n",
      "interpretation of the Transformer architecture makes it more challenging to\n",
      "improve the Transformer architecture. In this paper, we first interpret the\n",
      "Transformer architecture comprehensively in plain words based on our\n",
      "understanding and experiences. The interpretations are further proved and\n",
      "verified. These interpretations also cover the Extractor, a family of drop-in\n",
      "replacements for the multi-head self-attention in the Transformer architecture.\n",
      "Then, we propose an improvement on a type of the Extractor that outperforms the\n",
      "self-attention, without introducing additional trainable parameters.\n",
      "Experimental results demonstrate that the improved Extractor performs even\n",
      "better, showing a way to improve the Transformer architecture.\n",
      "\n",
      "Title: Atleus: Accelerating Transformers on the Edge Enabled by 3D Heterogeneous Manycore Architectures\n",
      "Summary: Transformer architectures have become the standard neural network model for\n",
      "various machine learning applications including natural language processing and\n",
      "computer vision. However, the compute and memory requirements introduced by\n",
      "transformer models make them challenging to adopt for edge applications.\n",
      "Furthermore, fine-tuning pre-trained transformers (e.g., foundation models) is\n",
      "a common task to enhance the model's predictive performance on specific\n",
      "tasks/applications. Existing transformer accelerators are oblivious to\n",
      "complexities introduced by fine-tuning. In this paper, we propose the design of\n",
      "a three-dimensional (3D) heterogeneous architecture referred to as Atleus that\n",
      "incorporates heterogeneous computing resources specifically optimized to\n",
      "accelerate transformer models for the dual purposes of fine-tuning and\n",
      "inference. Specifically, Atleus utilizes non-volatile memory and systolic array\n",
      "for accelerating transformer computational kernels using an integrated 3D\n",
      "platform. Moreover, we design a suitable NoC to achieve high performance and\n",
      "energy efficiency. Finally, Atleus adopts an effective quantization scheme to\n",
      "support model compression. Experimental results demonstrate that Atleus\n",
      "outperforms existing state-of-the-art by up to 56x and 64.5x in terms of\n",
      "performance and energy efficiency respectively\n",
      "\n",
      "Title: Spherical Position Encoding for Transformers\n",
      "Summary: Position encoding is the primary mechanism which induces notion of sequential\n",
      "order for input tokens in transformer architectures. Even though this\n",
      "formulation in the original transformer paper has yielded plausible performance\n",
      "for general purpose language understanding and generation, several new\n",
      "frameworks such as Rotary Position Embedding (RoPE) are proposed for further\n",
      "enhancement. In this paper, we introduce the notion of \"geotokens\" which are\n",
      "input elements for transformer architectures, each representing an information\n",
      "related to a geological location. Unlike the natural language the sequential\n",
      "position is not important for the model but the geographical coordinates are.\n",
      "In order to induce the concept of relative position for such a setting and\n",
      "maintain the proportion between the physical distance and distance on embedding\n",
      "space, we formulate a position encoding mechanism based on RoPE architecture\n",
      "which is adjusted for spherical coordinates.\n",
      "\n",
      "Title: Equivariant Neural Functional Networks for Transformers\n",
      "Summary: This paper systematically explores neural functional networks (NFN) for\n",
      "transformer architectures. NFN are specialized neural networks that treat the\n",
      "weights, gradients, or sparsity patterns of a deep neural network (DNN) as\n",
      "input data and have proven valuable for tasks such as learnable optimizers,\n",
      "implicit data representations, and weight editing. While NFN have been\n",
      "extensively developed for MLP and CNN, no prior work has addressed their design\n",
      "for transformers, despite the importance of transformers in modern deep\n",
      "learning. This paper aims to address this gap by providing a systematic study\n",
      "of NFN for transformers. We first determine the maximal symmetric group of the\n",
      "weights in a multi-head attention module as well as a necessary and sufficient\n",
      "condition under which two sets of hyperparameters of the multi-head attention\n",
      "module define the same function. We then define the weight space of transformer\n",
      "architectures and its associated group action, which leads to the design\n",
      "principles for NFN in transformers. Based on these, we introduce\n",
      "Transformer-NFN, an NFN that is equivariant under this group action.\n",
      "Additionally, we release a dataset of more than 125,000 Transformers model\n",
      "checkpoints trained on two datasets with two different tasks, providing a\n",
      "benchmark for evaluating Transformer-NFN and encouraging further research on\n",
      "transformer training and performance.\n",
      "```\n",
      "\n",
      "### Question:\n",
      "Which paper introduced the transformer architecture\n",
      "\n",
      "### Answer:\n",
      "The transformer architecture was introduced in the following paper:\n",
      "\n",
      "Title: Attention Is All You Need\n",
      "Authors: Vaswani, A., Uszkoreit, J., Martinez, J., Gomez, P., Polosukhin, I., Schuster, M., et al.\n",
      "Year: 2017\n",
      "\n",
      "This paper proposes the transformer model, which uses self-attention mechanisms instead of recurrence or convolution to allow the model to directly focus on relevant parts of the input sequence for computation.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Combine summaries into a context string\n",
    "context = \"\\n\\n\".join(\n",
    "    f\"Title: {paper['title']}\\nSummary: {paper['summary']}\" for paper in top_papers\n",
    ")\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "You are a helpful AI QA assistant, for answering querries about research methods.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "```\n",
    "{context}\n",
    "```\n",
    "\n",
    "### Question:\n",
    "{question}\n",
    "\n",
    "### Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=PROMPT_TEMPLATE.strip(),\n",
    ")\n",
    "\n",
    "# Define the Hugging Face pipeline for text generation\n",
    "generate_text = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=model,  # Replace with your model\n",
    "    tokenizer=tokenizer,  # Replace with your tokenizer\n",
    "    return_full_text=True,\n",
    "    max_new_tokens=8192,\n",
    "    repetition_penalty=1.1,\n",
    ")\n",
    "\n",
    "# Wrap the Hugging Face pipeline into LangChain's LLM\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)\n",
    "\n",
    "# Create the LLMChain with the prompt template and the LLM\n",
    "qa_chain = LLMChain(prompt=prompt_template, llm=llm)\n",
    "\n",
    "# Ask the model a question and get the answer\n",
    "question = user_query # TODO maybe change this, to make it different than the search (or change the search)\n",
    "response = qa_chain.run({\"context\": context, \"question\": question})\n",
    "\n",
    "# Print the response\n",
    "print(\"Answer:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54446b81",
   "metadata": {},
   "source": [
    "## Simpler model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bba74e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "\n",
    "model_id = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
    "\n",
    "rag = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Combine summaries into a context string, but make sure it's within the token limit\n",
    "context = \"\\n\\n\".join(\n",
    "    f\"Title: {paper['title']}\\nSummary: {paper['summary']}\" for paper in top_papers\n",
    ")\n",
    "\n",
    "# Encode the context and check its length\n",
    "input_ids = tokenizer.encode(context, return_tensors=\"pt\")\n",
    "max_length = 1700  # Adjust this based on your model's max token length\n",
    "\n",
    "# Truncate if necessary to fit within the max token limit\n",
    "if input_ids.shape[1] > max_length:\n",
    "    input_ids = input_ids[:, :max_length]\n",
    "\n",
    "\n",
    "# Prepare the prompt, ensuring it stays within the token limit\n",
    "prompt = f\"\"\"Here are some research papers:\n",
    "\n",
    "{context[:max_length]}  # Only include a truncated context if necessary\n",
    "\n",
    "Use the above research paper summaries to answer the following question:\n",
    "\n",
    "Question: {user_query}\n",
    "Answer:\"\"\"\n",
    "\n",
    "# Generate the answer using the same prompt\n",
    "output = rag(prompt, max_new_tokens=300)\n",
    "\n",
    "# Provide the generated answer along with the papers\n",
    "print(\"Research Papers and Generated Answer:\")\n",
    "print(f\"Research Papers:\\n{context[:max_length]}\")  # Display truncated context\n",
    "print(f\"Generated Answer:\\n{output[0]['generated_text']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
